nohup: 忽略输入
starting loading data
2018-11-30 11:53:07
finish loading data
2018-11-30 11:58:47
batch_num 15625
configurations {'keep_prob': 0.7, 'data_path': '../../data/ubuntu/data.pkl', 'word_layers_itg': 2, 'evaluate_step': 15625, 'filter_size': 3, 'filter_h': 3, 'hidden_embedding_dim': 200, 'emb_train': False, 'embedding_file': '../../data/ubuntu/word_embedding.pkl', 'CPU': '/cpu:0', 'epoch': 4, 'lr': 0.001, 'save_path': 'Gcnn_v3_test/version4/', 'word_layers_enc': 2, 'print_step': 1562, '_EOS_': 28270, 'word_embedding_dim': 200, 'batch_size': 64, 'max_turn_num': 8, 'final_n_class': 1, 'word_layers_agg': 2, 'max_turn_len': 50, 'train_steps': 62500, 'output_path': 'Gcnn_v3_output/version4/', 'init_model': 'Gcnn_v3_model/version4/'}
2018-11-30 11:59:10.770041: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-30 11:59:10.770057: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-30 11:59:10.770061: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-11-30 11:59:10.770064: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-30 11:59:10.770067: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-11-30 11:59:10.878156: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-30 11:59:10.878589: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:02:00.0
Total memory: 7.92GiB
Free memory: 7.83GiB
2018-11-30 11:59:10.878599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-11-30 11:59:10.878603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-11-30 11:59:10.878609: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:02:00.0)
starting shuffle train data
finish building train data
2018-11-30 12:00:18.184807: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1617 get requests, put_count=1219 evicted_count=1000 eviction_rate=0.820345 and unsatisfied allocation rate=0.926407
2018-11-30 12:00:18.184841: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2018-11-30 12:00:21.956432: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1618 get requests, put_count=1346 evicted_count=1000 eviction_rate=0.742942 and unsatisfied allocation rate=0.797899
2018-11-30 12:00:21.956473: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 212 to 233
2018-11-30 12:00:25.923195: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1623 get requests, put_count=1612 evicted_count=1000 eviction_rate=0.620347 and unsatisfied allocation rate=0.647566
2018-11-30 12:00:25.923235: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 449 to 493
2018-11-30 12:00:33.740998: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4863 get requests, put_count=4842 evicted_count=1000 eviction_rate=0.206526 and unsatisfied allocation rate=0.231544
2018-11-30 12:00:33.741035: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1158 to 1273
epoch=1 step: 1562 loss 0.32315746 processed: [0.099968]
epoch=1 step: 3124 loss 0.28342277 processed: [0.199936]
epoch=1 step: 4686 loss 0.27362263 processed: [0.299904]
epoch=1 step: 6248 loss 0.17933547 processed: [0.399872]
epoch=1 step: 7810 loss 0.2300779 processed: [0.49984]
epoch=1 step: 9372 loss 0.25389776 processed: [0.599808]
epoch=1 step: 10934 loss 0.26965436 processed: [0.699776]
epoch=1 step: 12496 loss 0.279401 processed: [0.799744]
epoch=1 step: 14058 loss 0.31673735 processed: [0.899712]
epoch=1 step: 15620 loss 0.21446314 processed: [0.99968]
('R10_1', 0.7633257995479729)
('R10_2', 0.8683921035262115)
('R10_5', 0.9658779526771606)
('R2_1', 0.9353161189671381)
save evaluate_step: 1
2018-11-30 14:53:38
starting shuffle train data
finish building train data
epoch=2 step: 17182 loss 0.2368451 processed: [1.099648]
epoch=2 step: 18744 loss 0.2390722 processed: [1.199616]
epoch=2 step: 20306 loss 0.34152076 processed: [1.299584]
epoch=2 step: 21868 loss 0.31696552 processed: [1.399552]
epoch=2 step: 23430 loss 0.22678718 processed: [1.49952]
epoch=2 step: 24992 loss 0.29279467 processed: [1.599488]
epoch=2 step: 26554 loss 0.20717812 processed: [1.699456]
epoch=2 step: 28116 loss 0.32507277 processed: [1.799424]
epoch=2 step: 29678 loss 0.34621963 processed: [1.899392]
epoch=2 step: 31240 loss 0.26562738 processed: [1.99936]
('R10_1', 0.772786367182031)
('R10_2', 0.8747324839490369)
('R10_5', 0.9688781326879613)
('R2_1', 0.939596375782547)
save evaluate_step: 2
2018-11-30 17:49:29
0.33328637
epoch=2 save model
learning rate 0.001
2018-11-30 17:49:33
starting shuffle train data
finish building train data
epoch=3 step: 32802 loss 0.22904712 processed: [2.099328]
epoch=3 step: 34364 loss 0.3337423 processed: [2.199296]
epoch=3 step: 35926 loss 0.26653522 processed: [2.299264]
epoch=3 step: 37488 loss 0.16159144 processed: [2.399232]
epoch=3 step: 39050 loss 0.24091151 processed: [2.4992]
epoch=3 step: 40612 loss 0.40913063 processed: [2.599168]
epoch=3 step: 42174 loss 0.2937551 processed: [2.699136]
epoch=3 step: 43736 loss 0.24431628 processed: [2.799104]
epoch=3 step: 45298 loss 0.26549435 processed: [2.899072]
epoch=3 step: 46860 loss 0.20865938 processed: [2.99904]
('R10_1', 0.7710862651759105)
('R10_2', 0.8765125907554453)
('R10_5', 0.9695781746904815)
('R2_1', 0.9394763685821149)
save evaluate_step: 3
2018-11-30 20:44:48
0.21005991
epoch=3 save model
learning rate 0.00075
2018-11-30 20:44:52
starting shuffle train data
finish building train data
epoch=4 step: 48422 loss 0.2205231 processed: [3.099008]
epoch=4 step: 49984 loss 0.19648106 processed: [3.198976]
epoch=4 step: 51546 loss 0.24749878 processed: [3.298944]
epoch=4 step: 53108 loss 0.22743353 processed: [3.398912]
epoch=4 step: 54670 loss 0.21715331 processed: [3.49888]
epoch=4 step: 56232 loss 0.18458886 processed: [3.598848]
epoch=4 step: 57794 loss 0.14905354 processed: [3.698816]
epoch=4 step: 59356 loss 0.29577887 processed: [3.798784]
epoch=4 step: 60918 loss 0.2181145 processed: [3.898752]
epoch=4 step: 62480 loss 0.23671257 processed: [3.99872]
('R10_1', 0.7679660779646779)
('R10_2', 0.8726523591415485)
('R10_5', 0.9682980978858732)
('R2_1', 0.9391963517811068)
save evaluate_step: 4
2018-11-30 23:38:40
0.21862191
epoch=4 save model
learning rate 0.00075
2018-11-30 23:38:44
train time:11.7602 h
