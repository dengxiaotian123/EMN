nohup: 忽略输入
starting loading data
2018-12-21 12:05:04
finish loading data
2018-12-21 12:10:31
batch_num 15625
configurations {'keep_prob': 0.7, 'data_path': '../../data/ubuntu/data.pkl', 'word_layers_itg': 2, 'evaluate_step': 15625, 'filter_size': 3, 'filter_h': 3, 'hidden_embedding_dim': 200, 'emb_train': False, 'embedding_file': '../../data/ubuntu/word_embedding.pkl', 'CPU': '/cpu:0', 'epoch': 5, 'lr': 0.001, 'save_path': 'Gcnn_v13_test/version/', 'word_layers_enc': 2, 'print_step': 1562, '_EOS_': 28270, 'word_embedding_dim': 200, 'batch_size': 64, 'max_turn_num': 10, 'final_n_class': 1, 'word_layers_agg': 2, 'max_turn_len': 50, 'train_steps': 78125, 'output_path': 'Gcnn_v13_output/version/', 'init_model': 'Gcnn_v13_model/version/'}
2018-12-21 12:10:58.983157: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-12-21 12:10:58.983175: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-12-21 12:10:58.983179: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-12-21 12:10:58.983182: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-12-21 12:10:58.983185: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-12-21 12:11:00.099422: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-21 12:11:00.099798: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:02:00.0
Total memory: 7.92GiB
Free memory: 7.83GiB
2018-12-21 12:11:00.099810: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-12-21 12:11:00.099815: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-12-21 12:11:00.099821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:02:00.0)
starting shuffle train data
finish building train data
2018-12-21 12:12:11.739843: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.78GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-12-21 12:12:11.834592: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1962 get requests, put_count=1243 evicted_count=1000 eviction_rate=0.804505 and unsatisfied allocation rate=0.927115
2018-12-21 12:12:11.834609: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2018-12-21 12:12:15.386463: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1968 get requests, put_count=2342 evicted_count=2000 eviction_rate=0.853971 and unsatisfied allocation rate=0.83435
2018-12-21 12:12:15.386493: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 176 to 193
2018-12-21 12:12:19.501141: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1969 get requests, put_count=1517 evicted_count=1000 eviction_rate=0.659196 and unsatisfied allocation rate=0.752666
2018-12-21 12:12:19.501192: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 339 to 372
2018-12-21 12:12:23.753559: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1963 get requests, put_count=1859 evicted_count=1000 eviction_rate=0.537924 and unsatisfied allocation rate=0.592461
2018-12-21 12:12:23.753587: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
2018-12-21 12:12:38.344946: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 15726 get requests, put_count=15883 evicted_count=1000 eviction_rate=0.0629604 and unsatisfied allocation rate=0.0633982
2018-12-21 12:12:38.344975: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1694 to 1863
epoch=1 step: 1562 loss 0.37272084 processed: [0.099968]
epoch=1 step: 3124 loss 0.30065554 processed: [0.199936]
epoch=1 step: 4686 loss 0.5964015 processed: [0.299904]
epoch=1 step: 6248 loss 0.42227715 processed: [0.399872]
epoch=1 step: 7810 loss 0.42453638 processed: [0.49984]
epoch=1 step: 9372 loss 0.4201586 processed: [0.599808]
epoch=1 step: 10934 loss 0.36033297 processed: [0.699776]
epoch=1 step: 12496 loss 0.37362596 processed: [0.799744]
epoch=1 step: 14058 loss 0.396305 processed: [0.899712]
epoch=1 step: 15620 loss 0.3206284 processed: [0.99968]
('R10_1', 0.7450047002820169)
('R10_2', 0.8553913234794087)
('R10_5', 0.9621377282636958)
('R2_1', 0.9292357541452487)
save evaluate_step: 1
2018-12-21 15:47:00
starting shuffle train data
finish building train data
epoch=2 step: 17182 loss 0.420536 processed: [1.099648]
epoch=2 step: 18744 loss 0.2918295 processed: [1.199616]
epoch=2 step: 20306 loss 0.3186812 processed: [1.299584]
epoch=2 step: 21868 loss 0.3773085 processed: [1.399552]
epoch=2 step: 23430 loss 0.25565633 processed: [1.49952]
epoch=2 step: 24992 loss 0.3514105 processed: [1.599488]
epoch=2 step: 26554 loss 0.38536206 processed: [1.699456]
epoch=2 step: 28116 loss 0.36489397 processed: [1.799424]
epoch=2 step: 29678 loss 0.29238746 processed: [1.899392]
epoch=2 step: 31240 loss 0.36719638 processed: [1.99936]
('R10_1', 0.7508850531031862)
('R10_2', 0.8604116246974819)
('R10_5', 0.9650579034742085)
('R2_1', 0.9325159509570574)
save evaluate_step: 2
2018-12-21 19:21:58
0.25547495
epoch=2 save model
learning rate 0.001
2018-12-21 19:22:01
starting shuffle train data
finish building train data
epoch=3 step: 32802 loss 0.26167718 processed: [2.099328]
epoch=3 step: 34364 loss 0.20655015 processed: [2.199296]
epoch=3 step: 35926 loss 0.40774843 processed: [2.299264]
epoch=3 step: 37488 loss 0.20399483 processed: [2.399232]
epoch=3 step: 39050 loss 0.39706132 processed: [2.4992]
epoch=3 step: 40612 loss 0.29087722 processed: [2.599168]
epoch=3 step: 42174 loss 0.3012344 processed: [2.699136]
epoch=3 step: 43736 loss 0.31353015 processed: [2.799104]
epoch=3 step: 45298 loss 0.2773926 processed: [2.899072]
epoch=3 step: 46860 loss 0.2490947 processed: [2.99904]
('R10_1', 0.7566053963237794)
('R10_2', 0.8627717663059784)
('R10_5', 0.9651979118747125)
('R2_1', 0.9338360301618097)
save evaluate_step: 3
2018-12-21 22:57:14
0.2755271
epoch=3 save model
learning rate 0.00075
2018-12-21 22:57:17
starting shuffle train data
finish building train data
epoch=4 step: 48422 loss 0.23054957 processed: [3.099008]
epoch=4 step: 49984 loss 0.22082607 processed: [3.198976]
epoch=4 step: 51546 loss 0.19973963 processed: [3.298944]
epoch=4 step: 53108 loss 0.31958717 processed: [3.398912]
epoch=4 step: 54670 loss 0.15871146 processed: [3.49888]
epoch=4 step: 56232 loss 0.25568676 processed: [3.598848]
epoch=4 step: 57794 loss 0.20369464 processed: [3.698816]
epoch=4 step: 59356 loss 0.282817 processed: [3.798784]
epoch=4 step: 60918 loss 0.26618052 processed: [3.898752]
epoch=4 step: 62480 loss 0.19432813 processed: [3.99872]
('R10_1', 0.7507250435026102)
('R10_2', 0.8573914434866092)
('R10_5', 0.9620777246634798)
('R2_1', 0.930495829749785)
save evaluate_step: 4
2018-12-22 02:31:44
0.29218495
epoch=4 save model
learning rate 0.00075
2018-12-22 02:31:47
starting shuffle train data
finish building train data
epoch=5 step: 64042 loss 0.27962315 processed: [4.098688]
epoch=5 step: 65604 loss 0.28925917 processed: [4.198656]
epoch=5 step: 67166 loss 0.24083357 processed: [4.298624]
epoch=5 step: 68728 loss 0.14244764 processed: [4.398592]
epoch=5 step: 70290 loss 0.22687979 processed: [4.49856]
epoch=5 step: 71852 loss 0.13038628 processed: [4.598528]
epoch=5 step: 73414 loss 0.15999797 processed: [4.698496]
epoch=5 step: 74976 loss 0.16493516 processed: [4.798464]
epoch=5 step: 76538 loss 0.17873272 processed: [4.898432]
epoch=5 step: 78100 loss 0.20617466 processed: [4.9984]
('R10_1', 0.740784447066824)
('R10_2', 0.8534512070724244)
('R10_5', 0.9607576454587275)
('R2_1', 0.9272956377382643)
save evaluate_step: 5
2018-12-22 06:06:18
0.22126037
epoch=5 save model
learning rate 0.000375
2018-12-22 06:06:19
train time:18.0209 h
