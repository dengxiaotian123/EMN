nohup: 忽略输入
starting loading data
2018-11-22 00:49:00
finish loading data
2018-11-22 00:51:38
batch_num 12500
configurations {'keep_prob': 0.8, 'data_path': '../../data/douban/data.pkl', 'word_layers_itg': 2, 'evaluate_step': 12500, 'filter_size': 3, 'filter_h': 3, 'hidden_embedding_dim': 200, 'emb_train': False, 'embedding_file': '../../data/douban/word_embedding.pkl', 'CPU': '/cpu:0', 'epoch': 10, 'lr': 0.001, 'save_path': 'Gcnn_v3_test/version3/', 'word_layers_enc': 2, 'print_step': 1250, '_EOS_': 1, 'word_embedding_dim': 200, 'batch_size': 80, 'max_turn_num': 9, 'final_n_class': 1, 'word_layers_agg': 2, 'max_turn_len': 50, 'train_steps': 125000, 'output_path': 'Gcnn_v3_output/version3/', 'init_model': 'Gcnn_v3_model/version3/'}
2018-11-22 00:51:39.070572: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-22 00:51:39.070590: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-22 00:51:39.070594: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-11-22 00:51:39.070597: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-22 00:51:39.070600: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-11-22 00:51:39.185711: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-22 00:51:39.186155: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:02:00.0
Total memory: 7.92GiB
Free memory: 7.83GiB
2018-11-22 00:51:39.186166: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-11-22 00:51:39.186170: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-11-22 00:51:39.186176: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:02:00.0)
starting shuffle train data
finish building train data
2018-11-22 00:53:07.024175: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.79GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-11-22 00:53:07.119570: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1816 get requests, put_count=1237 evicted_count=1000 eviction_rate=0.808407 and unsatisfied allocation rate=0.924559
2018-11-22 00:53:07.119603: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2018-11-22 00:53:11.704058: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1816 get requests, put_count=1347 evicted_count=1000 eviction_rate=0.74239 and unsatisfied allocation rate=0.818282
2018-11-22 00:53:11.704105: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 193 to 212
2018-11-22 00:53:16.415676: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 1814 get requests, put_count=1537 evicted_count=1000 eviction_rate=0.650618 and unsatisfied allocation rate=0.722161
2018-11-22 00:53:16.415706: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 372 to 409
2018-11-22 00:53:23.119668: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 3641 get requests, put_count=3099 evicted_count=1000 eviction_rate=0.322685 and unsatisfied allocation rate=0.445207
2018-11-22 00:53:23.119701: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 871 to 958
epoch=0 step: 1250 loss 0.37192613 processed: [0.1]
epoch=0 step: 2500 loss 0.4013509 processed: [0.2]
epoch=0 step: 3750 loss 0.2986677 processed: [0.3]
epoch=0 step: 5000 loss 0.3071375 processed: [0.4]
epoch=0 step: 6250 loss 0.3054593 processed: [0.5]
epoch=0 step: 7500 loss 0.40906084 processed: [0.6]
epoch=0 step: 8750 loss 0.3586473 processed: [0.7]
epoch=0 step: 10000 loss 0.29252815 processed: [0.8]
epoch=0 step: 11250 loss 0.37052757 processed: [0.9]
epoch=0 step: 12500 loss 0.38602543 processed: [1.0]
total num: 664
MAP: 0.533523851405
MRR: 0.576979345955
P@1: 0.387048192771
('R10_1:', 0.2288248231019314)
('R10_2', 0.40390968636450536)
('R10_5', 0.744634490342322)
('ave:', 0.4791533983233891)
save evaluate_step: 1
2018-11-22 03:20:34
starting shuffle train data
finish building train data
epoch=1 step: 13750 loss 0.30342355 processed: [1.1]
epoch=1 step: 15000 loss 0.26746604 processed: [1.2]
epoch=1 step: 16250 loss 0.27706105 processed: [1.3]
epoch=1 step: 17500 loss 0.24863291 processed: [1.4]
epoch=1 step: 18750 loss 0.28466102 processed: [1.5]
epoch=1 step: 20000 loss 0.30376932 processed: [1.6]
epoch=1 step: 21250 loss 0.3309164 processed: [1.7]
epoch=1 step: 22500 loss 0.3387546 processed: [1.8]
epoch=1 step: 23750 loss 0.33759618 processed: [1.9]
epoch=1 step: 25000 loss 0.26255438 processed: [2.0]
total num: 664
MAP: 0.534060940237
MRR: 0.582282104609
P@1: 0.393072289157
('R10_1:', 0.2269315356664753)
('R10_2', 0.41113860202715596)
('R10_5', 0.7629846768024481)
('ave:', 0.4850783580830628)
save evaluate_step: 2
2018-11-22 05:49:19
0.26076904
epoch=2 save model
learning rate 0.001
2018-11-22 05:49:21
starting shuffle train data
finish building train data
epoch=2 step: 26250 loss 0.35059783 processed: [2.1]
epoch=2 step: 27500 loss 0.19680761 processed: [2.2]
epoch=2 step: 28750 loss 0.3252783 processed: [2.3]
epoch=2 step: 30000 loss 0.18556702 processed: [2.4]
epoch=2 step: 31250 loss 0.27806744 processed: [2.5]
epoch=2 step: 32500 loss 0.28648096 processed: [2.6]
epoch=2 step: 33750 loss 0.30002078 processed: [2.7]
epoch=2 step: 35000 loss 0.35776377 processed: [2.8]
epoch=2 step: 36250 loss 0.36626145 processed: [2.9]
epoch=2 step: 37500 loss 0.2573859 processed: [3.0]
total num: 664
MAP: 0.540318907907
MRR: 0.579926850258
P@1: 0.382530120482
('R10_1:', 0.2222987186842608)
('R10_2', 0.41989864218779854)
('R10_5', 0.7677878179384205)
('ave:', 0.4854601762428903)
save evaluate_step: 3
2018-11-22 08:18:03
0.2661344
epoch=3 save model
learning rate 0.00075
2018-11-22 08:18:05
starting shuffle train data
finish building train data
epoch=3 step: 38750 loss 0.2513162 processed: [3.1]
epoch=3 step: 40000 loss 0.25353253 processed: [3.2]
epoch=3 step: 41250 loss 0.19927214 processed: [3.3]
epoch=3 step: 42500 loss 0.23305395 processed: [3.4]
epoch=3 step: 43750 loss 0.25030178 processed: [3.5]
epoch=3 step: 45000 loss 0.15772824 processed: [3.6]
epoch=3 step: 46250 loss 0.26714778 processed: [3.7]
epoch=3 step: 47500 loss 0.2379148 processed: [3.8]
epoch=3 step: 48750 loss 0.23026732 processed: [3.9]
epoch=3 step: 50000 loss 0.23660892 processed: [4.0]
total num: 664
MAP: 0.540947164271
MRR: 0.586561723083
P@1: 0.388554216867
('R10_1:', 0.22105804169057178)
('R10_2', 0.4291642761522277)
('R10_5', 0.7814926372155289)
('ave:', 0.49129634321330395)
save evaluate_step: 4
2018-11-22 10:47:56
0.23721337
epoch=4 save model
learning rate 0.00075
2018-11-22 10:47:58
starting shuffle train data
finish building train data
epoch=4 step: 51250 loss 0.14836136 processed: [4.1]
epoch=4 step: 52500 loss 0.09802486 processed: [4.2]
epoch=4 step: 53750 loss 0.18820687 processed: [4.3]
epoch=4 step: 55000 loss 0.19391307 processed: [4.4]
epoch=4 step: 56250 loss 0.10067198 processed: [4.5]
epoch=4 step: 57500 loss 0.12617737 processed: [4.6]
epoch=4 step: 58750 loss 0.15714191 processed: [4.7]
epoch=4 step: 60000 loss 0.22913176 processed: [4.8]
epoch=4 step: 61250 loss 0.15347445 processed: [4.9]
epoch=4 step: 62500 loss 0.11568645 processed: [5.0]
total num: 664
MAP: 0.541684996553
MRR: 0.587642833238
P@1: 0.397590361446
('R10_1:', 0.22996868426085282)
('R10_2', 0.4150937081659971)
('R10_5', 0.7719186268885068)
('ave:', 0.4906498684252629)
save evaluate_step: 5
2018-11-22 13:21:41
0.11636434
epoch=5 save model
learning rate 0.000375
2018-11-22 13:21:43
starting shuffle train data
finish building train data
epoch=5 step: 63750 loss 0.1441388 processed: [5.1]
epoch=5 step: 65000 loss 0.18015823 processed: [5.2]
epoch=5 step: 66250 loss 0.06324976 processed: [5.3]
epoch=5 step: 67500 loss 0.06683528 processed: [5.4]
epoch=5 step: 68750 loss 0.12340182 processed: [5.5]
epoch=5 step: 70000 loss 0.10736863 processed: [5.6]
epoch=5 step: 71250 loss 0.13935336 processed: [5.7]
epoch=5 step: 72500 loss 0.17525569 processed: [5.8]
epoch=5 step: 73750 loss 0.09068452 processed: [5.9]
epoch=5 step: 75000 loss 0.11027635 processed: [6.0]
total num: 664
MAP: 0.544565227795
MRR: 0.590868234844
P@1: 0.403614457831
('R10_1:', 0.23589237903996932)
('R10_2', 0.4105397781602599)
('R10_5', 0.7950468540829989)
('ave:', 0.49675448862564114)
save evaluate_step: 6
2018-11-22 15:54:56
0.10158246
epoch=6 save model
learning rate 0.000375
2018-11-22 15:54:57
starting shuffle train data
finish building train data
epoch=6 step: 76250 loss 0.03317824 processed: [6.1]
epoch=6 step: 77500 loss 0.11875643 processed: [6.2]
epoch=6 step: 78750 loss 0.06317514 processed: [6.3]
epoch=6 step: 80000 loss 0.0934507 processed: [6.4]
epoch=6 step: 81250 loss 0.19589967 processed: [6.5]
epoch=6 step: 82500 loss 0.051768072 processed: [6.6]
epoch=6 step: 83750 loss 0.05649039 processed: [6.7]
epoch=6 step: 85000 loss 0.07872814 processed: [6.8]
epoch=6 step: 86250 loss 0.029273735 processed: [6.9]
epoch=6 step: 87500 loss 0.06766359 processed: [7.0]
total num: 664
MAP: 0.534679516441
MRR: 0.581367135207
P@1: 0.39156626506
('R10_1:', 0.2274837445018167)
('R10_2', 0.39505283036909555)
('R10_5', 0.7806392235609104)
('ave:', 0.4851314525233712)
save evaluate_step: 7
2018-11-22 18:26:58
0.072002195
epoch=7 save model
learning rate 0.0001875
2018-11-22 18:27:00
starting shuffle train data
finish building train data
epoch=7 step: 88750 loss 0.007955184 processed: [7.1]
epoch=7 step: 90000 loss 0.01515392 processed: [7.2]
epoch=7 step: 91250 loss 0.012983749 processed: [7.3]
epoch=7 step: 92500 loss 0.036094554 processed: [7.4]
epoch=7 step: 93750 loss 0.008507721 processed: [7.5]
epoch=7 step: 95000 loss 0.08581179 processed: [7.6]
epoch=7 step: 96250 loss 0.09779068 processed: [7.7]
epoch=7 step: 97500 loss 0.033977292 processed: [7.8]
epoch=7 step: 98750 loss 0.029333513 processed: [7.9]
epoch=7 step: 100000 loss 0.09398133 processed: [8.0]
total num: 664
MAP: 0.541869863003
MRR: 0.590139725569
P@1: 0.405120481928
('R10_1:', 0.23995864410021023)
('R10_2', 0.4060217058711034)
('R10_5', 0.7695699464524769)
('ave:', 0.49211339448718266)
save evaluate_step: 8
2018-11-22 20:57:54
0.09652063
epoch=8 save model
learning rate 0.0001875
2018-11-22 20:57:55
starting shuffle train data
finish building train data
epoch=8 step: 101250 loss 0.029406074 processed: [8.1]
epoch=8 step: 102500 loss 0.01672623 processed: [8.2]
epoch=8 step: 103750 loss 0.010171227 processed: [8.3]
epoch=8 step: 105000 loss 0.022978807 processed: [8.4]
epoch=8 step: 106250 loss 0.0637508 processed: [8.5]
epoch=8 step: 107500 loss 0.020255622 processed: [8.6]
epoch=8 step: 108750 loss 0.05385454 processed: [8.7]
epoch=8 step: 110000 loss 0.03512562 processed: [8.8]
epoch=8 step: 111250 loss 0.023296343 processed: [8.9]
epoch=8 step: 112500 loss 0.015480011 processed: [9.0]
total num: 664
MAP: 0.546579133759
MRR: 0.589929957927
P@1: 0.402108433735
('R10_1:', 0.2431463951042263)
('R10_2', 0.39931989864218803)
('R10_5', 0.784447313061771)
('ave:', 0.4942551887048696)
save evaluate_step: 9
2018-11-22 23:28:51
0.016381774
epoch=9 save model
learning rate 9.375e-05
2018-11-22 23:28:52
starting shuffle train data
finish building train data
epoch=9 step: 113750 loss 0.005997234 processed: [9.1]
epoch=9 step: 115000 loss 0.011785109 processed: [9.2]
epoch=9 step: 116250 loss 0.0025497566 processed: [9.3]
epoch=9 step: 117500 loss 0.0018370118 processed: [9.4]
epoch=9 step: 118750 loss 0.0020952593 processed: [9.5]
epoch=9 step: 120000 loss 0.005449249 processed: [9.6]
epoch=9 step: 121250 loss 0.01684252 processed: [9.7]
epoch=9 step: 122500 loss 0.0010705478 processed: [9.8]
epoch=9 step: 123750 loss 0.06972471 processed: [9.9]
epoch=9 step: 125000 loss 0.0039744703 processed: [10.0]
total num: 664
MAP: 0.545309710118
MRR: 0.587805985848
P@1: 0.397590361446
('R10_1:', 0.23908013004398534)
('R10_2', 0.40853174603174613)
('R10_5', 0.7801694874737045)
('ave:', 0.4930812368268738)
save evaluate_step: 10
2018-11-23 01:57:43
0.0022605583
epoch=10 save model
learning rate 9.375e-05
2018-11-23 01:57:45
train time:25.1457 h
