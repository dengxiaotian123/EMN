nohup: 忽略输入
starting loading data
2018-11-23 13:21:50
finish loading data
2018-11-23 13:24:25
batch_num 15625
configurations {'keep_prob': 0.8, 'data_path': '../../data/douban/data.pkl', 'word_layers_itg': 2, 'evaluate_step': 15625, 'filter_size': 3, 'filter_h': 3, 'hidden_embedding_dim': 200, 'emb_train': False, 'embedding_file': '../../data/douban/word_embedding.pkl', 'CPU': '/cpu:0', 'epoch': 12, 'lr': 0.0005, 'save_path': 'Gcnn_v3_test/version_2/', 'word_layers_enc': 2, 'print_step': 1562, '_EOS_': 1, 'word_embedding_dim': 200, 'batch_size': 64, 'max_turn_num': 10, 'final_n_class': 1, 'word_layers_agg': 2, 'max_turn_len': 50, 'train_steps': 187500, 'output_path': 'Gcnn_v3_output/version_2/', 'init_model': 'Gcnn_v3_model/version_2/'}
2018-11-23 13:24:26.071649: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-23 13:24:26.071668: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-23 13:24:26.071672: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-11-23 13:24:26.071675: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-23 13:24:26.071678: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-11-23 13:24:26.181990: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-23 13:24:26.182423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:02:00.0
Total memory: 7.92GiB
Free memory: 7.83GiB
2018-11-23 13:24:26.182435: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-11-23 13:24:26.182440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-11-23 13:24:26.182446: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:02:00.0)
starting shuffle train data
finish building train data
2018-11-23 13:25:54.170385: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2005 get requests, put_count=1248 evicted_count=1000 eviction_rate=0.801282 and unsatisfied allocation rate=0.926185
2018-11-23 13:25:54.170414: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2018-11-23 13:25:57.676946: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2012 get requests, put_count=2347 evicted_count=2000 eviction_rate=0.852152 and unsatisfied allocation rate=0.835487
2018-11-23 13:25:57.676982: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 176 to 193
2018-11-23 13:26:01.294555: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1030 evicted_count=1000 eviction_rate=0.970874 and unsatisfied allocation rate=0
2018-11-23 13:26:06.019150: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2009 get requests, put_count=1866 evicted_count=1000 eviction_rate=0.535906 and unsatisfied allocation rate=0.598308
2018-11-23 13:26:06.019202: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
2018-11-23 13:26:15.099268: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 8041 get requests, put_count=7826 evicted_count=1000 eviction_rate=0.127779 and unsatisfied allocation rate=0.168511
2018-11-23 13:26:15.099307: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1540 to 1694
epoch=0 step: 1562 loss 0.42277533 processed: [0.099968]
epoch=0 step: 3124 loss 0.38837135 processed: [0.199936]
epoch=0 step: 4686 loss 0.4398508 processed: [0.299904]
epoch=0 step: 6248 loss 0.36967406 processed: [0.399872]
epoch=0 step: 7810 loss 0.30382854 processed: [0.49984]
epoch=0 step: 9372 loss 0.40558484 processed: [0.599808]
epoch=0 step: 10934 loss 0.32653004 processed: [0.699776]
epoch=0 step: 12496 loss 0.30566594 processed: [0.799744]
epoch=0 step: 14058 loss 0.38883847 processed: [0.899712]
epoch=0 step: 15620 loss 0.30880666 processed: [0.99968]
total num: 665
MAP: 0.529442415326
MRR: 0.573820264948
P@1: 0.384962406015
('R10_1:', 0.22468552333213984)
('R10_2', 0.3911612364243943)
('R10_5', 0.7609476071130209)
('ave:', 0.47750324219309914)
save evaluate_step: 1
2018-11-23 16:11:18
starting shuffle train data
finish building train data
epoch=1 step: 17182 loss 0.27375793 processed: [1.099648]
epoch=1 step: 18744 loss 0.2803951 processed: [1.199616]
epoch=1 step: 20306 loss 0.26173395 processed: [1.299584]
epoch=1 step: 21868 loss 0.3467687 processed: [1.399552]
epoch=1 step: 23430 loss 0.43829808 processed: [1.49952]
epoch=1 step: 24992 loss 0.27090234 processed: [1.599488]
epoch=1 step: 26554 loss 0.2832024 processed: [1.699456]
epoch=1 step: 28116 loss 0.36603403 processed: [1.799424]
epoch=1 step: 29678 loss 0.35327637 processed: [1.899392]
epoch=1 step: 31240 loss 0.33537316 processed: [1.99936]
total num: 665
MAP: 0.538211384233
MRR: 0.58164160401
P@1: 0.396992481203
('R10_1:', 0.2304356128416278)
('R10_2', 0.4097577276524643)
('R10_5', 0.7553944384771454)
('ave:', 0.48540554140295294)
save evaluate_step: 2
2018-11-23 18:57:55
0.32471108
epoch=2 save model
learning rate 0.0005
2018-11-23 18:57:56
starting shuffle train data
finish building train data
epoch=2 step: 32802 loss 0.20942834 processed: [2.099328]
epoch=2 step: 34364 loss 0.21976373 processed: [2.199296]
epoch=2 step: 35926 loss 0.36101562 processed: [2.299264]
epoch=2 step: 37488 loss 0.31199598 processed: [2.399232]
epoch=2 step: 39050 loss 0.28945857 processed: [2.4992]
epoch=2 step: 40612 loss 0.25679854 processed: [2.599168]
epoch=2 step: 42174 loss 0.17360964 processed: [2.699136]
epoch=2 step: 43736 loss 0.21563007 processed: [2.799104]
epoch=2 step: 45298 loss 0.19421092 processed: [2.899072]
epoch=2 step: 46860 loss 0.23304513 processed: [2.99904]
total num: 665
MAP: 0.544327375743
MRR: 0.585010144409
P@1: 0.396992481203
('R10_1:', 0.23283088674066107)
('R10_2', 0.42635994748776684)
('R10_5', 0.7590535863468196)
('ave:', 0.4907624036549547)
save evaluate_step: 3
2018-11-23 21:44:41
0.25583234
epoch=3 save model
learning rate 0.000375
2018-11-23 21:44:42
starting shuffle train data
finish building train data
epoch=3 step: 48422 loss 0.18298471 processed: [3.099008]
epoch=3 step: 49984 loss 0.22021589 processed: [3.198976]
epoch=3 step: 51546 loss 0.22458133 processed: [3.298944]
epoch=3 step: 53108 loss 0.21911371 processed: [3.398912]
epoch=3 step: 54670 loss 0.24870217 processed: [3.49888]
epoch=3 step: 56232 loss 0.23362103 processed: [3.598848]
epoch=3 step: 57794 loss 0.21023162 processed: [3.698816]
epoch=3 step: 59356 loss 0.19559518 processed: [3.798784]
epoch=3 step: 60918 loss 0.30615523 processed: [3.898752]
epoch=3 step: 62480 loss 0.32147223 processed: [3.99872]
total num: 665
MAP: 0.537725750506
MRR: 0.582116004296
P@1: 0.393984962406
('R10_1:', 0.2295584198591716)
('R10_2', 0.40330230337749123)
('R10_5', 0.7741329514261844)
('ave:', 0.48680339864526484)
save evaluate_step: 4
2018-11-24 00:31:45
0.27297914
epoch=4 save model
learning rate 0.000375
2018-11-24 00:31:46
starting shuffle train data
finish building train data
epoch=4 step: 64042 loss 0.25285938 processed: [4.098688]
epoch=4 step: 65604 loss 0.28457522 processed: [4.198656]
epoch=4 step: 67166 loss 0.21430163 processed: [4.298624]
epoch=4 step: 68728 loss 0.07657783 processed: [4.398592]
epoch=4 step: 70290 loss 0.16124876 processed: [4.49856]
epoch=4 step: 71852 loss 0.21452019 processed: [4.598528]
epoch=4 step: 73414 loss 0.079133466 processed: [4.698496]
epoch=4 step: 74976 loss 0.22265652 processed: [4.798464]
epoch=4 step: 76538 loss 0.20847908 processed: [4.898432]
epoch=4 step: 78100 loss 0.14570324 processed: [4.9984]
total num: 665
MAP: 0.544713341366
MRR: 0.592379758921
P@1: 0.404511278195
('R10_1:', 0.23732784341806887)
('R10_2', 0.419768468791025)
('R10_5', 0.778791025182003)
('ave:', 0.4962486193122007)
save evaluate_step: 5
2018-11-24 03:17:50
0.2110658
epoch=5 save model
learning rate 0.0001875
2018-11-24 03:17:52
starting shuffle train data
finish building train data
epoch=5 step: 79662 loss 0.10732265 processed: [5.098368]
epoch=5 step: 81224 loss 0.08307427 processed: [5.198336]
epoch=5 step: 82786 loss 0.12839596 processed: [5.298304]
epoch=5 step: 84348 loss 0.18420477 processed: [5.398272]
epoch=5 step: 85910 loss 0.20864475 processed: [5.49824]
epoch=5 step: 87472 loss 0.15364864 processed: [5.598208]
epoch=5 step: 89034 loss 0.118185684 processed: [5.698176]
epoch=5 step: 90596 loss 0.25314263 processed: [5.798144]
epoch=5 step: 92158 loss 0.16196093 processed: [5.898112]
epoch=5 step: 93720 loss 0.17401554 processed: [5.99808]
total num: 665
MAP: 0.532605194723
MRR: 0.573919322115
P@1: 0.381954887218
('R10_1:', 0.22253013486096188)
('R10_2', 0.39540756653538606)
('R10_5', 0.757044993435971)
('ave:', 0.47724368314809434)
save evaluate_step: 6
2018-11-24 06:03:49
0.1026975
epoch=6 save model
learning rate 0.0001875
2018-11-24 06:03:50
starting shuffle train data
finish building train data
epoch=6 step: 95282 loss 0.05996546 processed: [6.098048]
epoch=6 step: 96844 loss 0.054479625 processed: [6.198016]
epoch=6 step: 98406 loss 0.04333278 processed: [6.297984]
epoch=6 step: 99968 loss 0.07773252 processed: [6.397952]
epoch=6 step: 101530 loss 0.113188565 processed: [6.49792]
epoch=6 step: 103092 loss 0.08286746 processed: [6.597888]
epoch=6 step: 104654 loss 0.12223317 processed: [6.697856]
epoch=6 step: 106216 loss 0.08892529 processed: [6.797824]
epoch=6 step: 107778 loss 0.043936834 processed: [6.897792]
epoch=6 step: 109340 loss 0.17276184 processed: [6.99776]
total num: 665
MAP: 0.536437414161
MRR: 0.579323905001
P@1: 0.392481203008
('R10_1:', 0.23052512232963351)
('R10_2', 0.3945088912758086)
('R10_5', 0.7698162071846283)
('ave:', 0.4838487904931766)
save evaluate_step: 7
2018-11-24 08:49:28
0.10788904
epoch=7 save model
learning rate 9.375e-05
2018-11-24 08:49:29
starting shuffle train data
finish building train data
epoch=7 step: 110902 loss 0.06695224 processed: [7.097728]
epoch=7 step: 112464 loss 0.023445433 processed: [7.197696]
epoch=7 step: 114026 loss 0.07528289 processed: [7.297664]
epoch=7 step: 115588 loss 0.027601458 processed: [7.397632]
epoch=7 step: 117150 loss 0.07134515 processed: [7.4976]
epoch=7 step: 118712 loss 0.022230614 processed: [7.597568]
epoch=7 step: 120274 loss 0.07413537 processed: [7.697536]
epoch=7 step: 121836 loss 0.055450656 processed: [7.797504]
epoch=7 step: 123398 loss 0.046004932 processed: [7.897472]
epoch=7 step: 124960 loss 0.08123176 processed: [7.99744]
total num: 665
MAP: 0.540278098639
MRR: 0.586094402673
P@1: 0.398496240602
('R10_1:', 0.23649003461033524)
('R10_2', 0.40804272586227464)
('R10_5', 0.7638680033416877)
('ave:', 0.4888782509546099)
save evaluate_step: 8
2018-11-24 11:35:53
0.028694913
epoch=8 save model
learning rate 9.375e-05
2018-11-24 11:35:54
starting shuffle train data
finish building train data
epoch=8 step: 126522 loss 0.02070164 processed: [8.097408]
epoch=8 step: 128084 loss 0.034119107 processed: [8.197376]
epoch=8 step: 129646 loss 0.025857508 processed: [8.297344]
epoch=8 step: 131208 loss 0.014331631 processed: [8.397312]
epoch=8 step: 132770 loss 0.012911785 processed: [8.49728]
epoch=8 step: 134332 loss 0.03955888 processed: [8.597248]
epoch=8 step: 135894 loss 0.01385083 processed: [8.697216]
epoch=8 step: 137456 loss 0.011150513 processed: [8.797184]
epoch=8 step: 139018 loss 0.039737053 processed: [8.897152]
epoch=8 step: 140580 loss 0.059901364 processed: [8.99712]
total num: 665
MAP: 0.539461475164
MRR: 0.581001909536
P@1: 0.384962406015
('R10_1:', 0.23328201456021)
('R10_2', 0.4131555078171619)
('R10_5', 0.7743919322114814)
('ave:', 0.4877092075505594)
save evaluate_step: 9
2018-11-24 14:22:28
0.007645892
epoch=9 save model
learning rate 4.6875e-05
2018-11-24 14:22:29
starting shuffle train data
finish building train data
epoch=9 step: 142142 loss 0.008660214 processed: [9.097088]
epoch=9 step: 143704 loss 0.059353836 processed: [9.197056]
epoch=9 step: 145266 loss 0.071956046 processed: [9.297024]
epoch=9 step: 146828 loss 0.04982822 processed: [9.396992]
epoch=9 step: 148390 loss 0.010122456 processed: [9.49696]
epoch=9 step: 149952 loss 0.04219193 processed: [9.596928]
epoch=9 step: 151514 loss 0.023812003 processed: [9.696896]
epoch=9 step: 153076 loss 0.016400535 processed: [9.796864]
epoch=9 step: 154638 loss 0.0014736152 processed: [9.896832]
epoch=9 step: 156200 loss 0.012189055 processed: [9.9968]
total num: 665
MAP: 0.539068225884
MRR: 0.580303735529
P@1: 0.383458646617
('R10_1:', 0.232204320324621)
('R10_2', 0.41333094641365303)
('R10_5', 0.7642308151330708)
('ave:', 0.4854327816502075)
save evaluate_step: 10
2018-11-24 17:09:26
0.080257975
epoch=10 save model
learning rate 4.6875e-05
2018-11-24 17:09:28
starting shuffle train data
finish building train data
