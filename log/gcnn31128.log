nohup: 忽略输入
starting loading data
2018-11-28 21:38:59
finish loading data
2018-11-28 21:41:34
batch_num 15625
configurations {'keep_prob': 0.8, 'data_path': '../../data/douban/data.pkl', 'word_layers_itg': 2, 'evaluate_step': 15625, 'filter_size': 3, 'filter_h': 3, 'hidden_embedding_dim': 200, 'emb_train': False, 'embedding_file': '../../data/douban/word_embedding.pkl', 'CPU': '/cpu:0', 'epoch': 8, 'lr': 0.001, 'save_path': 'Gcnn_v3_test/version_2/', 'word_layers_enc': 2, 'print_step': 1562, '_EOS_': 1, 'word_embedding_dim': 200, 'batch_size': 64, 'max_turn_num': 10, 'final_n_class': 1, 'word_layers_agg': 2, 'max_turn_len': 50, 'train_steps': 125000, 'output_path': 'Gcnn_v3_output/version_2/', 'init_model': 'Gcnn_v3_model/version_2/'}
2018-11-28 21:41:34.622496: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-28 21:41:34.622514: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-28 21:41:34.622518: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-11-28 21:41:34.622521: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-28 21:41:34.622524: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-11-28 21:41:34.737432: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-28 21:41:34.737866: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:02:00.0
Total memory: 7.92GiB
Free memory: 7.83GiB
2018-11-28 21:41:34.737877: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-11-28 21:41:34.737881: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-11-28 21:41:34.737887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:02:00.0)
starting shuffle train data
finish building train data
2018-11-28 21:43:02.459167: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2006 get requests, put_count=1249 evicted_count=1000 eviction_rate=0.800641 and unsatisfied allocation rate=0.925723
2018-11-28 21:43:02.459197: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2018-11-28 21:43:05.376431: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2011 get requests, put_count=2346 evicted_count=2000 eviction_rate=0.852515 and unsatisfied allocation rate=0.835903
2018-11-28 21:43:05.376471: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 176 to 193
2018-11-28 21:43:08.278860: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1030 evicted_count=1000 eviction_rate=0.970874 and unsatisfied allocation rate=0
2018-11-28 21:43:12.089578: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2005 get requests, put_count=1855 evicted_count=1000 eviction_rate=0.539084 and unsatisfied allocation rate=0.602993
2018-11-28 21:43:12.089610: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
2018-11-28 21:43:19.364496: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 8037 get requests, put_count=7793 evicted_count=1000 eviction_rate=0.12832 and unsatisfied allocation rate=0.172204
2018-11-28 21:43:19.364534: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1540 to 1694
epoch=0 step: 1562 loss 0.359936 processed: [0.099968]
epoch=0 step: 3124 loss 0.33176538 processed: [0.199936]
epoch=0 step: 4686 loss 0.36919728 processed: [0.299904]
epoch=0 step: 6248 loss 0.2753687 processed: [0.399872]
epoch=0 step: 7810 loss 0.26217568 processed: [0.49984]
epoch=0 step: 9372 loss 0.47206116 processed: [0.599808]
epoch=0 step: 10934 loss 0.42147094 processed: [0.699776]
epoch=0 step: 12496 loss 0.51205856 processed: [0.799744]
epoch=0 step: 14058 loss 0.21007764 processed: [0.899712]
epoch=0 step: 15620 loss 0.44570082 processed: [0.99968]
val_loss 1.2302539
total num: 665
MAP: 0.534506458654
MRR: 0.578002148228
P@1: 0.387969924812
('R10_1:', 0.22425945816923257)
('R10_2', 0.4111254326291919)
('R10_5', 0.7487922186418428)
('ave:', 0.4807759401890337)
save evaluate_step: 1
2018-11-28 23:54:49
learning rate: 0.001
starting shuffle train data
finish building train data
epoch=1 step: 17182 loss 0.42607805 processed: [1.099648]
epoch=1 step: 18744 loss 0.25904337 processed: [1.199616]
epoch=1 step: 20306 loss 0.28280848 processed: [1.299584]
epoch=1 step: 21868 loss 0.2868675 processed: [1.399552]
epoch=1 step: 23430 loss 0.3805983 processed: [1.49952]
epoch=1 step: 24992 loss 0.3487204 processed: [1.599488]
epoch=1 step: 26554 loss 0.19125327 processed: [1.699456]
epoch=1 step: 28116 loss 0.25564992 processed: [1.799424]
epoch=1 step: 29678 loss 0.2192754 processed: [1.899392]
epoch=1 step: 31240 loss 0.3334722 processed: [1.99936]
val_loss 1.0712569
total num: 665
MAP: 0.547051281596
MRR: 0.590714882444
P@1: 0.4
('R10_1:', 0.23415920754266611)
('R10_2', 0.42975772765246434)
('R10_5', 0.7589700441580145)
('ave:', 0.49344219056554195)
save evaluate_step: 2
2018-11-29 02:07:11
learning rate: 0.001
0.22657302
epoch=2 save model
2018-11-29 02:07:13
starting shuffle train data
finish building train data
epoch=2 step: 32802 loss 0.16519076 processed: [2.099328]
epoch=2 step: 34364 loss 0.3358768 processed: [2.199296]
epoch=2 step: 35926 loss 0.2757421 processed: [2.299264]
epoch=2 step: 37488 loss 0.24337234 processed: [2.399232]
epoch=2 step: 39050 loss 0.2757281 processed: [2.4992]
epoch=2 step: 40612 loss 0.3048542 processed: [2.599168]
epoch=2 step: 42174 loss 0.25286937 processed: [2.699136]
epoch=2 step: 43736 loss 0.3133393 processed: [2.799104]
epoch=2 step: 45298 loss 0.35098606 processed: [2.899072]
epoch=2 step: 46860 loss 0.26028317 processed: [2.99904]
val_loss 1.2907214
total num: 665
MAP: 0.55431411271
MRR: 0.601445876596
P@1: 0.41954887218
('R10_1:', 0.2462895333572025)
('R10_2', 0.42641365318057034)
('R10_5', 0.7852643513545773)
('ave:', 0.5055460665631701)
save evaluate_step: 3
2018-11-29 04:19:21
learning rate: 0.00075
0.18596146
epoch=3 save model
2018-11-29 04:19:23
starting shuffle train data
finish building train data
epoch=3 step: 48422 loss 0.15963921 processed: [3.099008]
epoch=3 step: 49984 loss 0.11351442 processed: [3.198976]
epoch=3 step: 51546 loss 0.25422478 processed: [3.298944]
epoch=3 step: 53108 loss 0.27286112 processed: [3.398912]
epoch=3 step: 54670 loss 0.14549315 processed: [3.49888]
epoch=3 step: 56232 loss 0.15850493 processed: [3.598848]
epoch=3 step: 57794 loss 0.27180654 processed: [3.698816]
epoch=3 step: 59356 loss 0.1926409 processed: [3.798784]
epoch=3 step: 60918 loss 0.30275077 processed: [3.898752]
epoch=3 step: 62480 loss 0.31613857 processed: [3.99872]
val_loss 1.6878289
total num: 665
MAP: 0.533195778325
MRR: 0.576500179019
P@1: 0.389473684211
('R10_1:', 0.22601384413414483)
('R10_2', 0.3962740183792815)
('R10_5', 0.7675963718820864)
('ave:', 0.4815089793250446)
save evaluate_step: 4
2018-11-29 06:31:30
learning rate: 0.00075
0.21338142
epoch=4 save model
2018-11-29 06:31:32
starting shuffle train data
finish building train data
epoch=4 step: 64042 loss 0.091315106 processed: [4.098688]
epoch=4 step: 65604 loss 0.20194831 processed: [4.198656]
epoch=4 step: 67166 loss 0.088047326 processed: [4.298624]
epoch=4 step: 68728 loss 0.14502233 processed: [4.398592]
epoch=4 step: 70290 loss 0.1983652 processed: [4.49856]
epoch=4 step: 71852 loss 0.26006 processed: [4.598528]
epoch=4 step: 73414 loss 0.12276689 processed: [4.698496]
epoch=4 step: 74976 loss 0.1986889 processed: [4.798464]
epoch=4 step: 76538 loss 0.11604406 processed: [4.898432]
epoch=4 step: 78100 loss 0.19623348 processed: [4.9984]
val_loss 2.3664916
total num: 665
MAP: 0.53224948686
MRR: 0.578101802124
P@1: 0.395488721805
('R10_1:', 0.22659028523690167)
('R10_2', 0.391812865497076)
('R10_5', 0.7640052512232965)
('ave:', 0.4813747354576285)
save evaluate_step: 5
2018-11-29 08:43:37
learning rate: 0.000375
0.20293768
epoch=5 save model
2018-11-29 08:43:40
starting shuffle train data
finish building train data
epoch=5 step: 79662 loss 0.120613016 processed: [5.098368]
epoch=5 step: 81224 loss 0.035639428 processed: [5.198336]
epoch=5 step: 82786 loss 0.088179305 processed: [5.298304]
epoch=5 step: 84348 loss 0.059310086 processed: [5.398272]
epoch=5 step: 85910 loss 0.11322678 processed: [5.49824]
epoch=5 step: 87472 loss 0.09964337 processed: [5.598208]
epoch=5 step: 89034 loss 0.11788365 processed: [5.698176]
epoch=5 step: 90596 loss 0.07926621 processed: [5.798144]
epoch=5 step: 92158 loss 0.07516951 processed: [5.898112]
epoch=5 step: 93720 loss 0.121504545 processed: [5.99808]
val_loss 2.6584663
total num: 665
MAP: 0.529724608905
MRR: 0.575656999642
P@1: 0.389473684211
('R10_1:', 0.2230063253371523)
('R10_2', 0.3888411504952858)
('R10_5', 0.7690535863468198)
('ave:', 0.4792927258228083)
save evaluate_step: 6
2018-11-29 10:57:42
learning rate: 0.000375
0.1323488
epoch=6 save model
2018-11-29 10:57:44
starting shuffle train data
finish building train data
epoch=6 step: 95282 loss 0.028143972 processed: [6.098048]
epoch=6 step: 96844 loss 0.037476655 processed: [6.198016]
epoch=6 step: 98406 loss 0.106126145 processed: [6.297984]
epoch=6 step: 99968 loss 0.07833101 processed: [6.397952]
epoch=6 step: 101530 loss 0.08734025 processed: [6.49792]
epoch=6 step: 103092 loss 0.12050754 processed: [6.597888]
epoch=6 step: 104654 loss 0.083951086 processed: [6.697856]
epoch=6 step: 106216 loss 0.0634149 processed: [6.797824]
epoch=6 step: 107778 loss 0.1878348 processed: [6.897792]
epoch=6 step: 109340 loss 0.09579328 processed: [6.99776]
val_loss 3.9643326
total num: 665
MAP: 0.529847145973
MRR: 0.570794844253
P@1: 0.38045112782
('R10_1:', 0.22187850578828017)
('R10_2', 0.39501014440864063)
('R10_5', 0.7655662966941165)
('ave:', 0.4772580108229125)
save evaluate_step: 7
2018-11-29 13:12:47
learning rate: 0.0001875
0.08120489
epoch=7 save model
2018-11-29 13:12:48
starting shuffle train data
finish building train data
epoch=7 step: 110902 loss 0.07107419 processed: [7.097728]
epoch=7 step: 112464 loss 0.053933166 processed: [7.197696]
epoch=7 step: 114026 loss 0.14839575 processed: [7.297664]
epoch=7 step: 115588 loss 0.025144536 processed: [7.397632]
epoch=7 step: 117150 loss 0.075095624 processed: [7.4976]
epoch=7 step: 118712 loss 0.02986165 processed: [7.597568]
epoch=7 step: 120274 loss 0.0391647 processed: [7.697536]
epoch=7 step: 121836 loss 0.0072307168 processed: [7.797504]
epoch=7 step: 123398 loss 0.055355646 processed: [7.897472]
epoch=7 step: 124960 loss 0.08580184 processed: [7.99744]
val_loss 4.6880474
total num: 665
MAP: 0.53063268124
MRR: 0.57514918248
P@1: 0.387969924812
('R10_1:', 0.22556271631459596)
('R10_2', 0.39591240004773837)
('R10_5', 0.7677324263038552)
('ave:', 0.4804932218663942)
save evaluate_step: 8
2018-11-29 15:25:53
learning rate: 0.0001875
0.048485048
epoch=8 save model
2018-11-29 15:25:55
train time:17.7822 h
