nohup: 忽略输入
starting loading data
2018-12-06 19:13:36
finish loading data
2018-12-06 19:16:14
batch_num 15625
configurations {'keep_prob': 0.75, 'data_path': '../../data/douban/data.pkl', 'word_layers_itg': 2, 'evaluate_step': 15625, 'filter_size': 3, 'filter_h': 3, 'hidden_embedding_dim': 200, 'emb_train': False, 'embedding_file': '../../data/douban/word_embedding.pkl', 'CPU': '/cpu:0', 'epoch': 8, 'lr': 0.001, 'save_path': 'Gcnn_v3_test/version_4/', 'word_layers_enc': 2, 'print_step': 1562, '_EOS_': 1, 'word_embedding_dim': 200, 'batch_size': 64, 'max_turn_num': 10, 'final_n_class': 1, 'word_layers_agg': 2, 'max_turn_len': 50, 'train_steps': 125000, 'output_path': 'Gcnn_v3_output/version_4/', 'init_model': 'Gcnn_v3_model/version_4/'}
2018-12-06 19:16:14.802173: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-12-06 19:16:14.802191: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-12-06 19:16:14.802195: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-12-06 19:16:14.802198: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-12-06 19:16:14.802201: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-12-06 19:16:15.285929: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-12-06 19:16:15.287071: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.79GiB
2018-12-06 19:16:15.287115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-12-06 19:16:15.287134: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-12-06 19:16:15.287168: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
starting shuffle train data
finish building train data
2018-12-06 19:17:48.974714: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2007 get requests, put_count=1250 evicted_count=1000 eviction_rate=0.8 and unsatisfied allocation rate=0.925262
2018-12-06 19:17:48.974742: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2018-12-06 19:17:51.914966: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2014 get requests, put_count=2348 evicted_count=2000 eviction_rate=0.851789 and unsatisfied allocation rate=0.835154
2018-12-06 19:17:51.915012: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 176 to 193
2018-12-06 19:17:54.817040: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1030 evicted_count=1000 eviction_rate=0.970874 and unsatisfied allocation rate=0
2018-12-06 19:17:58.678466: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2012 get requests, put_count=1863 evicted_count=1000 eviction_rate=0.536769 and unsatisfied allocation rate=0.600398
2018-12-06 19:17:58.678535: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
2018-12-06 19:18:05.944859: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 8040 get requests, put_count=7811 evicted_count=1000 eviction_rate=0.128025 and unsatisfied allocation rate=0.170274
2018-12-06 19:18:05.944899: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1540 to 1694
epoch=0 step: 1562 loss 0.38822585 processed: [0.099968]
epoch=0 step: 3124 loss 0.30514437 processed: [0.199936]
epoch=0 step: 4686 loss 0.4592534 processed: [0.299904]
epoch=0 step: 6248 loss 0.38130513 processed: [0.399872]
epoch=0 step: 7810 loss 0.3315597 processed: [0.49984]
epoch=0 step: 9372 loss 0.31619638 processed: [0.599808]
epoch=0 step: 10934 loss 0.26757443 processed: [0.699776]
epoch=0 step: 12496 loss 0.29296166 processed: [0.799744]
epoch=0 step: 14058 loss 0.26100075 processed: [0.899712]
epoch=0 step: 15620 loss 0.34471476 processed: [0.99968]
val_loss 1.0752501
total num: 665
MAP: 0.530592529841
MRR: 0.572640529896
P@1: 0.381954887218
('R10_1:', 0.22526196443489668)
('R10_2', 0.39888053467000834)
('R10_5', 0.7557954409834112)
('ave:', 0.4775209811739601)
save evaluate_step: 1
2018-12-06 21:28:16
learning rate: 0.001
0.42209896
epoch=1 save model
2018-12-06 21:28:18
starting shuffle train data
finish building train data
epoch=1 step: 17182 loss 0.4498552 processed: [1.099648]
epoch=1 step: 18744 loss 0.23914513 processed: [1.199616]
epoch=1 step: 20306 loss 0.21722801 processed: [1.299584]
epoch=1 step: 21868 loss 0.39051104 processed: [1.399552]
epoch=1 step: 23430 loss 0.24828528 processed: [1.49952]
epoch=1 step: 24992 loss 0.30464286 processed: [1.599488]
epoch=1 step: 26554 loss 0.3465272 processed: [1.699456]
epoch=1 step: 28116 loss 0.28572768 processed: [1.799424]
epoch=1 step: 29678 loss 0.3281818 processed: [1.899392]
epoch=1 step: 31240 loss 0.28265524 processed: [1.99936]
val_loss 1.242913
total num: 665
MAP: 0.536287009805
MRR: 0.583567251462
P@1: 0.392481203008
('R10_1:', 0.22706647571309224)
('R10_2', 0.4165139038071367)
('R10_5', 0.7488906790786494)
('ave:', 0.4841344204789498)
save evaluate_step: 2
2018-12-06 23:39:33
learning rate: 0.001
0.21148938
epoch=2 save model
2018-12-06 23:39:34
starting shuffle train data
finish building train data
epoch=2 step: 32802 loss 0.23627473 processed: [2.099328]
epoch=2 step: 34364 loss 0.28258985 processed: [2.199296]
epoch=2 step: 35926 loss 0.1331565 processed: [2.299264]
epoch=2 step: 37488 loss 0.28560254 processed: [2.399232]
epoch=2 step: 39050 loss 0.28813857 processed: [2.4992]
epoch=2 step: 40612 loss 0.21011168 processed: [2.599168]
epoch=2 step: 42174 loss 0.36794275 processed: [2.699136]
epoch=2 step: 43736 loss 0.23959416 processed: [2.799104]
epoch=2 step: 45298 loss 0.2145395 processed: [2.899072]
epoch=2 step: 46860 loss 0.20578024 processed: [2.99904]
val_loss 1.0810277
total num: 665
MAP: 0.551898522999
MRR: 0.598921709034
P@1: 0.413533834586
('R10_1:', 0.24278076142737792)
('R10_2', 0.4336758563074351)
('R10_5', 0.7664094760711306)
('ave:', 0.5012033600709381)
save evaluate_step: 3
2018-12-07 01:50:16
learning rate: 0.00075
0.32105798
epoch=3 save model
2018-12-07 01:50:18
starting shuffle train data
finish building train data
epoch=3 step: 48422 loss 0.22270222 processed: [3.099008]
epoch=3 step: 49984 loss 0.1835789 processed: [3.198976]
epoch=3 step: 51546 loss 0.24144965 processed: [3.298944]
epoch=3 step: 53108 loss 0.19407426 processed: [3.398912]
epoch=3 step: 54670 loss 0.24185461 processed: [3.49888]
epoch=3 step: 56232 loss 0.33262008 processed: [3.598848]
epoch=3 step: 57794 loss 0.2758345 processed: [3.698816]
epoch=3 step: 59356 loss 0.18380174 processed: [3.798784]
epoch=3 step: 60918 loss 0.21020526 processed: [3.898752]
epoch=3 step: 62480 loss 0.24815731 processed: [3.99872]
val_loss 1.4866602
total num: 665
MAP: 0.549429973507
MRR: 0.594129967777
P@1: 0.413533834586
('R10_1:', 0.24598878147750322)
('R10_2', 0.4258706289533356)
('R10_5', 0.7596694116242992)
('ave:', 0.4981037663208789)
save evaluate_step: 4
2018-12-07 04:01:04
learning rate: 0.00075
0.20963132
epoch=4 save model
2018-12-07 04:01:05
starting shuffle train data
finish building train data
epoch=4 step: 64042 loss 0.22796546 processed: [4.098688]
epoch=4 step: 65604 loss 0.14658403 processed: [4.198656]
epoch=4 step: 67166 loss 0.17341559 processed: [4.298624]
epoch=4 step: 68728 loss 0.13919804 processed: [4.398592]
epoch=4 step: 70290 loss 0.1594019 processed: [4.49856]
epoch=4 step: 71852 loss 0.1422016 processed: [4.598528]
epoch=4 step: 73414 loss 0.13785672 processed: [4.698496]
epoch=4 step: 74976 loss 0.16205898 processed: [4.798464]
epoch=4 step: 76538 loss 0.25410587 processed: [4.898432]
epoch=4 step: 78100 loss 0.057938367 processed: [4.9984]
val_loss 2.0145445
total num: 665
MAP: 0.541725725643
MRR: 0.583228308867
P@1: 0.396992481203
('R10_1:', 0.23852010979830518)
('R10_2', 0.39412698412698427)
('R10_5', 0.7749182480009551)
('ave:', 0.4882519762731972)
save evaluate_step: 5
2018-12-07 06:11:49
learning rate: 0.000375
0.06629996
epoch=5 save model
2018-12-07 06:11:58
starting shuffle train data
finish building train data
epoch=5 step: 79662 loss 0.09537327 processed: [5.098368]
epoch=5 step: 81224 loss 0.20329294 processed: [5.198336]
epoch=5 step: 82786 loss 0.14955847 processed: [5.298304]
epoch=5 step: 84348 loss 0.11554152 processed: [5.398272]
epoch=5 step: 85910 loss 0.039549865 processed: [5.49824]
epoch=5 step: 87472 loss 0.0489874 processed: [5.598208]
epoch=5 step: 89034 loss 0.17527997 processed: [5.698176]
epoch=5 step: 90596 loss 0.1721306 processed: [5.798144]
epoch=5 step: 92158 loss 0.14517325 processed: [5.898112]
epoch=5 step: 93720 loss 0.07748374 processed: [5.99808]
val_loss 2.3798957
total num: 665
MAP: 0.53714007785
MRR: 0.575153956319
P@1: 0.381954887218
('R10_1:', 0.23022437044993424)
('R10_2', 0.4005489915264351)
('R10_5', 0.7635254803675859)
('ave:', 0.4814246272884907)
save evaluate_step: 6
2018-12-07 08:22:37
learning rate: 0.000375
0.17459202
epoch=6 save model
2018-12-07 08:22:38
starting shuffle train data
finish building train data
epoch=6 step: 95282 loss 0.06829012 processed: [6.098048]
epoch=6 step: 96844 loss 0.05375486 processed: [6.198016]
epoch=6 step: 98406 loss 0.12407713 processed: [6.297984]
epoch=6 step: 99968 loss 0.06727518 processed: [6.397952]
epoch=6 step: 101530 loss 0.055404022 processed: [6.49792]
epoch=6 step: 103092 loss 0.08373318 processed: [6.597888]
epoch=6 step: 104654 loss 0.091834635 processed: [6.697856]
epoch=6 step: 106216 loss 0.054057747 processed: [6.797824]
epoch=6 step: 107778 loss 0.103628956 processed: [6.897792]
epoch=6 step: 109340 loss 0.0708545 processed: [6.99776]
val_loss 3.6515841
total num: 665
MAP: 0.538361890886
MRR: 0.579086406492
P@1: 0.396992481203
('R10_1:', 0.23581334288101197)
('R10_2', 0.39459243346461387)
('R10_5', 0.7750543024227237)
('ave:', 0.48665014289155734)
save evaluate_step: 7
2018-12-07 10:33:49
learning rate: 0.0001875
0.06927976
epoch=7 save model
2018-12-07 10:33:51
starting shuffle train data
finish building train data
epoch=7 step: 110902 loss 0.06112287 processed: [7.097728]
epoch=7 step: 112464 loss 0.01908638 processed: [7.197696]
epoch=7 step: 114026 loss 0.029555634 processed: [7.297664]
epoch=7 step: 115588 loss 0.056958802 processed: [7.397632]
epoch=7 step: 117150 loss 0.077733755 processed: [7.4976]
epoch=7 step: 118712 loss 0.058109205 processed: [7.597568]
epoch=7 step: 120274 loss 0.05416631 processed: [7.697536]
epoch=7 step: 121836 loss 0.058543883 processed: [7.797504]
epoch=7 step: 123398 loss 0.07651754 processed: [7.897472]
epoch=7 step: 124960 loss 0.01655794 processed: [7.99744]
val_loss 4.7691917
total num: 665
MAP: 0.538432982731
MRR: 0.580782312925
P@1: 0.392481203008
('R10_1:', 0.23197875641484658)
('R10_2', 0.40183554123403736)
('R10_5', 0.7742791502565942)
('ave:', 0.4866316577614987)
save evaluate_step: 8
2018-12-07 12:45:56
learning rate: 0.0001875
0.096397914
epoch=8 save model
2018-12-07 12:45:58
train time:17.5394 h
