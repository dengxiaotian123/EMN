nohup: 忽略输入
starting loading data
2018-11-23 22:11:12
finish loading data
2018-11-23 22:13:51
batch_num 15625
configurations {'keep_prob': 0.7, 'data_path': '../../data/douban/data.pkl', 'word_layers_itg': 2, 'evaluate_step': 15625, 'filter_size': 2, 'filter_h': 3, 'hidden_embedding_dim': 200, 'emb_train': False, 'embedding_file': '../../data/douban/word_embedding.pkl', 'CPU': '/cpu:0', 'epoch': 11, 'lr': 0.001, 'save_path': 'Gcnn_v3_test/version_1/', 'word_layers_enc': 4, 'print_step': 1562, '_EOS_': 1, 'word_embedding_dim': 200, 'batch_size': 64, 'max_turn_num': 10, 'final_n_class': 1, 'word_layers_agg': 2, 'max_turn_len': 50, 'train_steps': 171875, 'output_path': 'Gcnn_v3_output/version_1/', 'init_model': 'Gcnn_v3_model/version_1/'}
2018-11-23 22:13:51.421534: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-23 22:13:51.421551: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-23 22:13:51.421555: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-11-23 22:13:51.421558: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-23 22:13:51.421561: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-11-23 22:13:51.538264: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-23 22:13:51.538698: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.79GiB
2018-11-23 22:13:51.538710: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-11-23 22:13:51.538714: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-11-23 22:13:51.538720: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
starting shuffle train data
finish building train data
2018-11-23 22:15:24.020186: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.78GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-11-23 22:15:24.098472: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2381 get requests, put_count=1250 evicted_count=1000 eviction_rate=0.8 and unsatisfied allocation rate=0.937001
2018-11-23 22:15:24.098502: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2018-11-23 22:15:27.637805: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2381 get requests, put_count=2324 evicted_count=2000 eviction_rate=0.860585 and unsatisfied allocation rate=0.869803
2018-11-23 22:15:27.637836: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 160 to 176
2018-11-23 22:15:31.252397: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2379 get requests, put_count=2421 evicted_count=2000 eviction_rate=0.826105 and unsatisfied allocation rate=0.832703
2018-11-23 22:15:31.252438: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 256 to 281
2018-11-23 22:15:34.931410: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2385 get requests, put_count=2600 evicted_count=2000 eviction_rate=0.769231 and unsatisfied allocation rate=0.763941
2018-11-23 22:15:34.931440: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 409 to 449
2018-11-23 22:15:39.549533: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1072 evicted_count=1000 eviction_rate=0.932836 and unsatisfied allocation rate=0
2018-11-23 22:15:47.578347: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4777 get requests, put_count=4854 evicted_count=1000 eviction_rate=0.206016 and unsatisfied allocation rate=0.225455
2018-11-23 22:15:47.578378: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1694 to 1863
epoch=0 step: 1562 loss 0.3467852 processed: [0.099968]
epoch=0 step: 3124 loss 0.47878748 processed: [0.199936]
epoch=0 step: 4686 loss 0.31108096 processed: [0.299904]
epoch=0 step: 6248 loss 0.34716263 processed: [0.399872]
epoch=0 step: 7810 loss 0.46132028 processed: [0.49984]
epoch=0 step: 9372 loss 0.3479681 processed: [0.599808]
epoch=0 step: 10934 loss 0.25971496 processed: [0.699776]
epoch=0 step: 12496 loss 0.26921892 processed: [0.799744]
epoch=0 step: 14058 loss 0.34648937 processed: [0.899712]
epoch=0 step: 15620 loss 0.30841538 processed: [0.99968]
total num: 665
MAP: 0.528213467105
MRR: 0.574544098341
P@1: 0.393984962406
('R10_1:', 0.23007399451008467)
('R10_2', 0.3853717627401838)
('R10_5', 0.7439408043919324)
('ave:', 0.4760215149157221)
save evaluate_step: 1
2018-11-24 01:34:38
learning rate: 0.001
starting shuffle train data
finish building train data
epoch=1 step: 17182 loss 0.2453594 processed: [1.099648]
epoch=1 step: 18744 loss 0.3951208 processed: [1.199616]
epoch=1 step: 20306 loss 0.28395766 processed: [1.299584]
epoch=1 step: 21868 loss 0.34909448 processed: [1.399552]
epoch=1 step: 23430 loss 0.23646703 processed: [1.49952]
epoch=1 step: 24992 loss 0.23241702 processed: [1.599488]
epoch=1 step: 26554 loss 0.5014685 processed: [1.699456]
epoch=1 step: 28116 loss 0.32729977 processed: [1.799424]
epoch=1 step: 29678 loss 0.26127124 processed: [1.899392]
epoch=1 step: 31240 loss 0.34110415 processed: [1.99936]
total num: 665
MAP: 0.539967759298
MRR: 0.582657835064
P@1: 0.389473684211
('R10_1:', 0.2326411266260889)
('R10_2', 0.41646019811433316)
('R10_5', 0.7494903926482878)
('ave:', 0.48511516599352994)
save evaluate_step: 2
2018-11-24 04:54:00
learning rate: 0.001
0.2293686
epoch=1 save model
2018-11-24 04:54:02
starting shuffle train data
finish building train data
epoch=2 step: 32802 loss 0.22702335 processed: [2.099328]
epoch=2 step: 34364 loss 0.1973674 processed: [2.199296]
epoch=2 step: 35926 loss 0.28604835 processed: [2.299264]
epoch=2 step: 37488 loss 0.27887398 processed: [2.399232]
epoch=2 step: 39050 loss 0.25605333 processed: [2.4992]
epoch=2 step: 40612 loss 0.23776872 processed: [2.599168]
epoch=2 step: 42174 loss 0.20810969 processed: [2.699136]
epoch=2 step: 43736 loss 0.24498329 processed: [2.799104]
epoch=2 step: 45298 loss 0.30645472 processed: [2.899072]
epoch=2 step: 46860 loss 0.17516044 processed: [2.99904]
total num: 665
MAP: 0.537837239546
MRR: 0.580307912639
P@1: 0.386466165414
('R10_1:', 0.22729203962286668)
('R10_2', 0.4111862990810359)
('R10_5', 0.7637904284520829)
('ave:', 0.4844800141257652)
save evaluate_step: 3
2018-11-24 08:13:00
learning rate: 0.00075
0.18932053
epoch=2 save model
2018-11-24 08:13:01
starting shuffle train data
finish building train data
epoch=3 step: 48422 loss 0.15682022 processed: [3.099008]
epoch=3 step: 49984 loss 0.14710385 processed: [3.198976]
epoch=3 step: 51546 loss 0.19911055 processed: [3.298944]
epoch=3 step: 53108 loss 0.1709404 processed: [3.398912]
epoch=3 step: 54670 loss 0.29361558 processed: [3.49888]
epoch=3 step: 56232 loss 0.28077397 processed: [3.598848]
epoch=3 step: 57794 loss 0.2103222 processed: [3.698816]
epoch=3 step: 59356 loss 0.3218323 processed: [3.798784]
epoch=3 step: 60918 loss 0.27259427 processed: [3.898752]
epoch=3 step: 62480 loss 0.18631065 processed: [3.99872]
total num: 665
MAP: 0.534846640174
MRR: 0.580736364721
P@1: 0.386466165414
('R10_1:', 0.22350757846998445)
('R10_2', 0.4012865497076024)
('R10_5', 0.7681907148824446)
('ave:', 0.4825056688947739)
save evaluate_step: 4
2018-11-24 11:32:53
learning rate: 0.00075
0.24931596
epoch=3 save model
2018-11-24 11:32:55
starting shuffle train data
finish building train data
epoch=4 step: 64042 loss 0.14544466 processed: [4.098688]
epoch=4 step: 65604 loss 0.12112284 processed: [4.198656]
epoch=4 step: 67166 loss 0.09420524 processed: [4.298624]
epoch=4 step: 68728 loss 0.13565153 processed: [4.398592]
epoch=4 step: 70290 loss 0.11421473 processed: [4.49856]
epoch=4 step: 71852 loss 0.087539434 processed: [4.598528]
epoch=4 step: 73414 loss 0.17244026 processed: [4.698496]
epoch=4 step: 74976 loss 0.15988621 processed: [4.798464]
epoch=4 step: 76538 loss 0.10473114 processed: [4.898432]
epoch=4 step: 78100 loss 0.11585012 processed: [4.9984]
total num: 665
MAP: 0.529109540906
MRR: 0.571828977205
P@1: 0.375939849624
('R10_1:', 0.2210765007757489)
('R10_2', 0.3956349206349206)
('R10_5', 0.7583034968373318)
('ave:', 0.47531554766388123)
save evaluate_step: 5
2018-11-24 14:53:40
learning rate: 0.000375
0.074670464
epoch=4 save model
2018-11-24 14:53:48
starting shuffle train data
finish building train data
epoch=5 step: 79662 loss 0.086509034 processed: [5.098368]
epoch=5 step: 81224 loss 0.11696445 processed: [5.198336]
epoch=5 step: 82786 loss 0.15014587 processed: [5.298304]
epoch=5 step: 84348 loss 0.13228974 processed: [5.398272]
