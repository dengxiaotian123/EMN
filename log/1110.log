nohup: 忽略输入
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
starting loading data
2018-11-10 15:51:41
finish loading data
2018-11-10 15:57:16
batch_num 12500
configurations {'data_path': '../../data/ubuntu/data.pkl', 'max_turn_num': 10, 'evaluate_step': 12500, 'filter_size': 4, 'filter_h': 3, 'train_steps': 62500, 'emb_train': False, 'CPU': '/cpu:0', 'embedding_file': '../../data/ubuntu/word_embedding.pkl', 'hidden_embedding_dim': 200, 'epoch': 5, 'print_step': 1250, 'save_path': 'Gcnn_v1_test/version2/', 'word_layers_enc': 2, '_EOS_': 28270, 'word_embedding_dim': 200, 'batch_size': 80, 'final_n_class': 1, 'word_layers_agg': 2, 'max_turn_len': 50, 'output_path': 'Gcnn_v1_output/version2/', 'init_model': 'Gcnn_v1_model/version2/'}
2018-11-10 15:57:44.287679: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-10 15:57:44.287696: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-10 15:57:44.287700: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-11-10 15:57:44.287704: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-10 15:57:44.287707: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-11-10 15:57:44.394277: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-10 15:57:44.394719: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:02:00.0
Total memory: 7.92GiB
Free memory: 7.83GiB
2018-11-10 15:57:44.394731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-11-10 15:57:44.394735: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-11-10 15:57:44.394741: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:02:00.0)
starting shuffle train data
finish building train data
2018-11-10 15:58:56.061452: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.79GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-11-10 15:58:56.160259: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2194 get requests, put_count=1461 evicted_count=1000 eviction_rate=0.684463 and unsatisfied allocation rate=0.83546
2018-11-10 15:58:56.160276: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2018-11-10 15:59:00.697925: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2196 get requests, put_count=2573 evicted_count=2000 eviction_rate=0.777303 and unsatisfied allocation rate=0.746357
2018-11-10 15:59:00.697957: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 176 to 193
2018-11-10 15:59:05.615620: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1030 evicted_count=1000 eviction_rate=0.970874 and unsatisfied allocation rate=0
2018-11-10 15:59:11.343124: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2197 get requests, put_count=2087 evicted_count=1000 eviction_rate=0.479157 and unsatisfied allocation rate=0.532089
2018-11-10 15:59:11.343162: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 655 to 720
2018-11-10 15:59:29.918953: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 17564 get requests, put_count=17683 evicted_count=1000 eviction_rate=0.0565515 and unsatisfied allocation rate=0.0589274
2018-11-10 15:59:29.918984: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1694 to 1863
epoch=0 step: 1250 loss 0.324858 processed: [0.1]
epoch=0 step: 2500 loss 0.26257515 processed: [0.2]
epoch=0 step: 3750 loss 0.32386485 processed: [0.3]
epoch=0 step: 5000 loss 0.33459353 processed: [0.4]
epoch=0 step: 6250 loss 0.2627192 processed: [0.5]
epoch=0 step: 7500 loss 0.34883565 processed: [0.6]
epoch=0 step: 8750 loss 0.24024534 processed: [0.7]
epoch=0 step: 10000 loss 0.2711982 processed: [0.8]
epoch=0 step: 11250 loss 0.5883932 processed: [0.9]
epoch=0 step: 12500 loss 0.40944988 processed: [1.0]
('R10_1', 0.75246)
('R2_1', 0.93252)
save evaluate_step: 1
2018-11-10 19:40:46
starting shuffle train data
finish building train data
epoch=1 step: 13750 loss 0.35219538 processed: [1.1]
epoch=1 step: 15000 loss 0.3174356 processed: [1.2]
epoch=1 step: 16250 loss 0.270348 processed: [1.3]
epoch=1 step: 17500 loss 0.23993394 processed: [1.4]
epoch=1 step: 18750 loss 0.1859746 processed: [1.5]
epoch=1 step: 20000 loss 0.2633887 processed: [1.6]
epoch=1 step: 21250 loss 0.38204974 processed: [1.7]
epoch=1 step: 22500 loss 0.22283894 processed: [1.8]
epoch=1 step: 23750 loss 0.33114988 processed: [1.9]
epoch=1 step: 25000 loss 0.30705106 processed: [2.0]
('R10_1', 0.76108)
('R2_1', 0.9363)
save evaluate_step: 2
2018-11-10 23:23:52
0.30705103
epoch=2 save model
2018-11-10 23:23:54
starting shuffle train data
finish building train data
epoch=2 step: 26250 loss 0.23837624 processed: [2.1]
epoch=2 step: 27500 loss 0.30090785 processed: [2.2]
epoch=2 step: 28750 loss 0.21516646 processed: [2.3]
epoch=2 step: 30000 loss 0.32071438 processed: [2.4]
epoch=2 step: 31250 loss 0.21096641 processed: [2.5]
epoch=2 step: 32500 loss 0.29235303 processed: [2.6]
epoch=2 step: 33750 loss 0.2873441 processed: [2.7]
epoch=2 step: 35000 loss 0.32894504 processed: [2.8]
epoch=2 step: 36250 loss 0.28267184 processed: [2.9]
epoch=2 step: 37500 loss 0.21372178 processed: [3.0]
('R10_1', 0.76386)
('R2_1', 0.93642)
save evaluate_step: 3
2018-11-11 03:06:37
0.2137218
epoch=3 save model
2018-11-11 03:06:39
starting shuffle train data
finish building train data
epoch=3 step: 38750 loss 0.31648418 processed: [3.1]
epoch=3 step: 40000 loss 0.2654469 processed: [3.2]
epoch=3 step: 41250 loss 0.314201 processed: [3.3]
epoch=3 step: 42500 loss 0.29386374 processed: [3.4]
epoch=3 step: 43750 loss 0.27724326 processed: [3.5]
epoch=3 step: 45000 loss 0.19251207 processed: [3.6]
epoch=3 step: 46250 loss 0.26153338 processed: [3.7]
epoch=3 step: 47500 loss 0.19252154 processed: [3.8]
epoch=3 step: 48750 loss 0.22605985 processed: [3.9]
epoch=3 step: 50000 loss 0.29396254 processed: [4.0]
('R10_1', 0.76438)
('R2_1', 0.93764)
save evaluate_step: 4
2018-11-11 06:48:55
0.29396254
epoch=4 save model
2018-11-11 06:48:56
starting shuffle train data
finish building train data
epoch=4 step: 51250 loss 0.31345183 processed: [4.1]
epoch=4 step: 52500 loss 0.21767455 processed: [4.2]
epoch=4 step: 53750 loss 0.10210012 processed: [4.3]
epoch=4 step: 55000 loss 0.21328813 processed: [4.4]
epoch=4 step: 56250 loss 0.25279522 processed: [4.5]
epoch=4 step: 57500 loss 0.32554436 processed: [4.6]
epoch=4 step: 58750 loss 0.2130012 processed: [4.7]
epoch=4 step: 60000 loss 0.349577 processed: [4.8]
epoch=4 step: 61250 loss 0.15639552 processed: [4.9]
epoch=4 step: 62500 loss 0.20174345 processed: [5.0]
('R10_1', 0.75774)
('R2_1', 0.93454)
save evaluate_step: 5
2018-11-11 10:31:37
0.20174345
epoch=5 save model
2018-11-11 10:31:38
train time:18.6660 h
