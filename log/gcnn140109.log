nohup: 忽略输入
starting loading data
2019-01-09 23:10:16
finish loading data
2019-01-09 23:12:50
batch_num 15625
configurations {'data_path': '../../data/douban/data.pkl', 'word_layers_itg': 2, 'evaluate_step': 15625, 'filter_size': 8, 'filter_h': 3, 'train_steps': 125000, 'emb_train': False, 'CPU': '/cpu:0', 'embedding_file': '../../data/douban/word_embedding.pkl', 'hidden_embedding_dim': 200, 'epoch': 8, 'lr': 0.001, 'save_path': 'Gcnn_v14_test/version/', 'word_layers_enc': 2, 'print_step': 1562, '_EOS_': 1, 'word_embedding_dim': 200, 'batch_size': 64, 'max_turn_num': 10, 'final_n_class': 1, 'word_layers_agg': 2, 'max_turn_len': 50, 'output_path': 'Gcnn_v14_output/version/', 'init_model': 'Gcnn_v14_model/version/'}
2019-01-09 23:12:50.331168: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2019-01-09 23:12:50.331184: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2019-01-09 23:12:50.331188: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2019-01-09 23:12:50.331191: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2019-01-09 23:12:50.331194: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2019-01-09 23:12:50.433132: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2019-01-09 23:12:50.433588: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.80GiB
2019-01-09 23:12:50.433599: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2019-01-09 23:12:50.433603: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2019-01-09 23:12:50.433608: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
starting shuffle train data
finish building train data
2019-01-09 23:14:15.580600: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2444 get requests, put_count=1455 evicted_count=1000 eviction_rate=0.687285 and unsatisfied allocation rate=0.854746
2019-01-09 23:14:15.580629: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2019-01-09 23:14:15.852155: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.78GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2019-01-09 23:14:18.124806: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 0 get requests, put_count=1019 evicted_count=1000 eviction_rate=0.981354 and unsatisfied allocation rate=0
2019-01-09 23:14:21.093810: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2445 get requests, put_count=2726 evicted_count=2000 eviction_rate=0.733676 and unsatisfied allocation rate=0.715337
2019-01-09 23:14:21.093843: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 339 to 372
2019-01-09 23:14:24.146240: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2448 get requests, put_count=3010 evicted_count=2000 eviction_rate=0.664452 and unsatisfied allocation rate=0.609477
2019-01-09 23:14:24.146280: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 596 to 655
2019-01-09 23:14:29.174946: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4897 get requests, put_count=4647 evicted_count=1000 eviction_rate=0.215193 and unsatisfied allocation rate=0.281193
2019-01-09 23:14:29.174975: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 1400 to 1540
epoch=1 step: 1562 loss 0.30385965 processed: [0.099968] gs 1562 learning_rate 0.00096000003
epoch=1 step: 3124 loss 0.37516326 processed: [0.199936] gs 3124 learning_rate 0.0008847359
epoch=1 step: 4686 loss 0.3795088 processed: [0.299904] gs 4686 learning_rate 0.0008153726
epoch=1 step: 6248 loss 0.5069705 processed: [0.399872] gs 6248 learning_rate 0.00075144734
epoch=1 step: 7810 loss 0.33313018 processed: [0.49984] gs 7810 learning_rate 0.0006925339
epoch=1 step: 9372 loss 0.3738776 processed: [0.599808] gs 9372 learning_rate 0.00063823926
epoch=1 step: 10934 loss 0.4606003 processed: [0.699776] gs 10934 learning_rate 0.0005882013
epoch=1 step: 12496 loss 0.3523412 processed: [0.799744] gs 12496 learning_rate 0.0005420862
epoch=1 step: 14058 loss 0.30422556 processed: [0.899712] gs 14058 learning_rate 0.00049958663
epoch=1 step: 15620 loss 0.33630228 processed: [0.99968] gs 15620 learning_rate 0.00046041902
total num: 665
MAP: 0.533600706462
MRR: 0.576165413534
P@1: 0.392481203008
('R10_1:', 0.23365795440983408)
('R10_2', 0.4004738035565102)
('R10_5', 0.7541914309583485)
('ave:', 0.4817617519879946)
save evaluate_step: 1
2019-01-10 01:32:51
starting shuffle train data
finish building train data
epoch=2 step: 17182 loss 0.35251802 processed: [1.099648] gs 17182 learning_rate 0.00042432215
epoch=2 step: 18744 loss 0.24301575 processed: [1.199616] gs 18744 learning_rate 0.00039105528
epoch=2 step: 20306 loss 0.27189252 processed: [1.299584] gs 20306 learning_rate 0.00036039655
epoch=2 step: 21868 loss 0.25320077 processed: [1.399552] gs 21868 learning_rate 0.0003321414
epoch=2 step: 23430 loss 0.28797555 processed: [1.49952] gs 23430 learning_rate 0.00030610152
epoch=2 step: 24992 loss 0.26417142 processed: [1.599488] gs 24992 learning_rate 0.00028210317
epoch=2 step: 26554 loss 0.24643993 processed: [1.699456] gs 26554 learning_rate 0.00025998626
epoch=2 step: 28116 loss 0.3387239 processed: [1.799424] gs 28116 learning_rate 0.00023960331
epoch=2 step: 29678 loss 0.27460116 processed: [1.899392] gs 29678 learning_rate 0.00022081842
epoch=2 step: 31240 loss 0.25454724 processed: [1.99936] gs 31240 learning_rate 0.00020350624
total num: 665
MAP: 0.53541637659
MRR: 0.581629669412
P@1: 0.393984962406
('R10_1:', 0.22638978398376888)
('R10_2', 0.40924573338107145)
('R10_5', 0.7560818713450295)
('ave:', 0.48379139951952177)
save evaluate_step: 2
2019-01-10 03:51:42
starting shuffle train data
finish building train data
epoch=3 step: 32802 loss 0.29273313 processed: [2.099328] gs 32802 learning_rate 0.00018755133
epoch=3 step: 34364 loss 0.21960482 processed: [2.199296] gs 34364 learning_rate 0.00018004929
epoch=3 step: 35926 loss 0.1822018 processed: [2.299264] gs 35926 learning_rate 0.00016593341
epoch=3 step: 37488 loss 0.33621556 processed: [2.399232] gs 37488 learning_rate 0.00015292423
epoch=3 step: 39050 loss 0.2320074 processed: [2.4992] gs 39050 learning_rate 0.00014093496
epoch=3 step: 40612 loss 0.27455047 processed: [2.599168] gs 40612 learning_rate 0.00012988565
epoch=3 step: 42174 loss 0.2937263 processed: [2.699136] gs 42174 learning_rate 0.00011970262
epoch=3 step: 43736 loss 0.24189302 processed: [2.799104] gs 43736 learning_rate 0.000110317924
epoch=3 step: 45298 loss 0.3232314 processed: [2.899072] gs 45298 learning_rate 0.000101669
epoch=3 step: 46860 loss 0.3009619 processed: [2.99904] gs 46860 learning_rate 9.369814e-05
total num: 665
MAP: 0.53993005023
MRR: 0.584063134026
P@1: 0.398496240602
('R10_1:', 0.23308151330707716)
('R10_2', 0.4222031268647807)
('R10_5', 0.7567335004177113)
('ave:', 0.4890845942410194)
save evaluate_step: 3
2019-01-10 06:09:58
0.17762047
epoch=2 save model
2019-01-10 06:10:00
starting shuffle train data
finish building train data
epoch=4 step: 48422 loss 0.2810105 processed: [3.099008] gs 48422 learning_rate 8.63522e-05
epoch=4 step: 49984 loss 0.094826326 processed: [3.198976] gs 49984 learning_rate 7.958218e-05
epoch=4 step: 51546 loss 0.32459867 processed: [3.298944] gs 51546 learning_rate 7.334295e-05
epoch=4 step: 53108 loss 0.21811505 processed: [3.398912] gs 53108 learning_rate 6.759285e-05
epoch=4 step: 54670 loss 0.24607545 processed: [3.49888] gs 54670 learning_rate 6.229357e-05
epoch=4 step: 56232 loss 0.3186915 processed: [3.598848] gs 56232 learning_rate 5.7409747e-05
epoch=4 step: 57794 loss 0.27661046 processed: [3.698816] gs 57794 learning_rate 5.2908825e-05
epoch=4 step: 59356 loss 0.112124 processed: [3.798784] gs 59356 learning_rate 4.8760765e-05
epoch=4 step: 60918 loss 0.18687382 processed: [3.898752] gs 60918 learning_rate 4.4937926e-05
epoch=4 step: 62480 loss 0.2660168 processed: [3.99872] gs 62480 learning_rate 4.1414787e-05
total num: 665
MAP: 0.547195556683
MRR: 0.592864900346
P@1: 0.407518796992
('R10_1:', 0.23872061105143802)
('R10_2', 0.4179926005489913)
('R10_5', 0.7736149898555916)
('ave:', 0.4963179092462194)
save evaluate_step: 4
2019-01-10 08:28:08
0.15398216
epoch=3 save model
2019-01-10 08:28:10
starting shuffle train data
finish building train data
epoch=5 step: 64042 loss 0.3617741 processed: [4.098688] gs 64042 learning_rate 3.8167866e-05
epoch=5 step: 65604 loss 0.19199757 processed: [4.198656] gs 65604 learning_rate 3.5175504e-05
epoch=5 step: 67166 loss 0.19851702 processed: [4.298624] gs 67166 learning_rate 3.3768483e-05
epoch=5 step: 68728 loss 0.1754618 processed: [4.398592] gs 68728 learning_rate 3.112103e-05
epoch=5 step: 70290 loss 0.3015055 processed: [4.49856] gs 70290 learning_rate 2.8681141e-05
epoch=5 step: 71852 loss 0.2922123 processed: [4.598528] gs 71852 learning_rate 2.6432539e-05
epoch=5 step: 73414 loss 0.33366793 processed: [4.698496] gs 73414 learning_rate 2.4360228e-05
epoch=5 step: 74976 loss 0.14193387 processed: [4.798464] gs 74976 learning_rate 2.2450386e-05
epoch=5 step: 76538 loss 0.24846204 processed: [4.898432] gs 76538 learning_rate 2.0690273e-05
epoch=5 step: 78100 loss 0.29841018 processed: [4.9984] gs 78100 learning_rate 1.9068155e-05
total num: 665
MAP: 0.547132004946
MRR: 0.592039026137
P@1: 0.407518796992
('R10_1:', 0.23947249075068613)
('R10_2', 0.41899510681465546)
('R10_5', 0.7731495405179618)
('ave:', 0.49638449435979703)
save evaluate_step: 5
2019-01-10 10:46:47
0.18248044
epoch=4 save model
2019-01-10 10:46:49
starting shuffle train data
finish building train data
epoch=6 step: 79662 loss 0.19427657 processed: [5.098368] gs 79662 learning_rate 1.7573211e-05
epoch=6 step: 81224 loss 0.24762794 processed: [5.198336] gs 81224 learning_rate 1.6195469e-05
epoch=6 step: 82786 loss 0.12213193 processed: [5.298304] gs 82786 learning_rate 1.4925744e-05
epoch=6 step: 84348 loss 0.18024802 processed: [5.398272] gs 84348 learning_rate 1.3755565e-05
epoch=6 step: 85910 loss 0.159071 processed: [5.49824] gs 85910 learning_rate 1.2677129e-05
epoch=6 step: 87472 loss 0.20865242 processed: [5.598208] gs 87472 learning_rate 1.1683241e-05
epoch=6 step: 89034 loss 0.2719974 processed: [5.698176] gs 89034 learning_rate 1.0767274e-05
epoch=6 step: 90596 loss 0.19337596 processed: [5.798144] gs 90596 learning_rate 9.923119e-06
epoch=6 step: 92158 loss 0.13521509 processed: [5.898112] gs 92158 learning_rate 9.145147e-06
epoch=6 step: 93720 loss 0.16628823 processed: [5.99808] gs 93720 learning_rate 8.428167e-06
total num: 665
MAP: 0.544216680922
MRR: 0.588245017305
P@1: 0.4
('R10_1:', 0.23553765365795434)
('R10_2', 0.4181179138321993)
('R10_5', 0.7692648287385132)
('ave:', 0.4925636824092858)
save evaluate_step: 6
2019-01-10 13:07:08
0.26372898
epoch=5 save model
2019-01-10 13:07:09
starting shuffle train data
finish building train data
epoch=7 step: 95282 loss 0.1872067 processed: [6.098048] gs 95282 learning_rate 7.7673985e-06
epoch=7 step: 96844 loss 0.20936626 processed: [6.198016] gs 96844 learning_rate 7.158434e-06
epoch=7 step: 98406 loss 0.2096605 processed: [6.297984] gs 98406 learning_rate 6.5972126e-06
epoch=7 step: 99968 loss 0.2561556 processed: [6.397952] gs 99968 learning_rate 6.333324e-06
epoch=7 step: 101530 loss 0.23172094 processed: [6.49792] gs 101530 learning_rate 5.8367905e-06
epoch=7 step: 103092 loss 0.16863105 processed: [6.597888] gs 103092 learning_rate 5.379187e-06
epoch=7 step: 104654 loss 0.265648 processed: [6.697856] gs 104654 learning_rate 4.9574583e-06
epoch=7 step: 106216 loss 0.30971262 processed: [6.797824] gs 106216 learning_rate 4.568793e-06
epoch=7 step: 107778 loss 0.1593268 processed: [6.897792] gs 107778 learning_rate 4.2105994e-06
epoch=7 step: 109340 loss 0.2505769 processed: [6.99776] gs 109340 learning_rate 3.8804883e-06
total num: 665
MAP: 0.544502514554
MRR: 0.588690177826
P@1: 0.401503759398
('R10_1:', 0.23704141305645057)
('R10_2', 0.4148096431555076)
('R10_5', 0.7711445279866336)
('ave:', 0.4929486726626971)
save evaluate_step: 7
2019-01-10 15:27:55
0.27147734
epoch=6 save model
2019-01-10 15:27:57
starting shuffle train data
finish building train data
epoch=8 step: 110902 loss 0.22968051 processed: [7.097728] gs 110902 learning_rate 3.576258e-06
epoch=8 step: 112464 loss 0.095895514 processed: [7.197696] gs 112464 learning_rate 3.295879e-06
