nohup: 忽略输入
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
(?, 50, 800)
starting loading data
2018-11-08 09:17:41
finish loading data
2018-11-08 09:23:22
batch_num 12500
configurations {'data_path': '../../data/ubuntu/data.pkl', 'max_turn_num': 10, 'evaluate_step': 12500, 'filter_size': 3, 'filter_h': 3, 'train_steps': 62500, 'emb_train': False, 'CPU': '/cpu:0', 'embedding_file': '../../data/ubuntu/word_embedding.pkl', 'hidden_embedding_dim': 200, 'epoch': 5, 'print_step': 1250, 'save_path': 'Gcnn_v2_test/version1/', 'word_layers_enc': 1, '_EOS_': 28270, 'word_embedding_dim': 200, 'batch_size': 80, 'final_n_class': 1, 'word_layers_agg': 2, 'max_turn_len': 50, 'output_path': 'Gcnn_v2_output/version1/', 'init_model': 'Gcnn_v2_model/version1/'}
2018-11-08 09:23:49.886793: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-08 09:23:49.886811: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-08 09:23:49.886815: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
2018-11-08 09:23:49.886818: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
2018-11-08 09:23:49.886821: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn't compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
2018-11-08 09:23:50.096728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:893] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero
2018-11-08 09:23:50.097192: I tensorflow/core/common_runtime/gpu/gpu_device.cc:940] Found device 0 with properties: 
name: GeForce GTX 1070
major: 6 minor: 1 memoryClockRate (GHz) 1.683
pciBusID 0000:01:00.0
Total memory: 7.92GiB
Free memory: 7.79GiB
2018-11-08 09:23:50.097204: I tensorflow/core/common_runtime/gpu/gpu_device.cc:961] DMA: 0 
2018-11-08 09:23:50.097208: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971] 0:   Y 
2018-11-08 09:23:50.097215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Creating TensorFlow device (/gpu:0) -> (device: 0, name: GeForce GTX 1070, pci bus id: 0000:01:00.0)
starting shuffle train data
finish building train data
2018-11-08 09:32:00.578019: W tensorflow/core/common_runtime/bfc_allocator.cc:217] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.79GiB. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory is available.
2018-11-08 09:32:00.701656: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2056 get requests, put_count=1471 evicted_count=1000 eviction_rate=0.67981 and unsatisfied allocation rate=0.819553
2018-11-08 09:32:00.701679: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 100 to 110
2018-11-08 09:32:06.089201: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2061 get requests, put_count=1626 evicted_count=1000 eviction_rate=0.615006 and unsatisfied allocation rate=0.704512
2018-11-08 09:32:06.089244: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 193 to 212
2018-11-08 09:32:11.451316: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 2058 get requests, put_count=2818 evicted_count=2000 eviction_rate=0.709723 and unsatisfied allocation rate=0.618562
2018-11-08 09:32:11.451348: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 372 to 409
2018-11-08 09:32:19.739915: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:247] PoolAllocator: After 4117 get requests, put_count=3830 evicted_count=1000 eviction_rate=0.261097 and unsatisfied allocation rate=0.333738
2018-11-08 09:32:19.739945: I tensorflow/core/common_runtime/gpu/pool_allocator.cc:259] Raising pool_size_limit_ from 958 to 1053
epoch=0 step: 1250 loss 0.46080995 processed: [0.1]
epoch=0 step: 2500 loss 0.366374 processed: [0.2]
epoch=0 step: 3750 loss 0.34299713 processed: [0.3]
epoch=0 step: 5000 loss 0.38623652 processed: [0.4]
epoch=0 step: 6250 loss 0.22205296 processed: [0.5]
epoch=0 step: 7500 loss 0.35677713 processed: [0.6]
epoch=0 step: 8750 loss 0.3127986 processed: [0.7]
epoch=0 step: 10000 loss 0.36187112 processed: [0.8]
epoch=0 step: 11250 loss 0.2891552 processed: [0.9]
epoch=0 step: 12500 loss 0.38205305 processed: [1.0]
('R10_1', 0.74312)
('R2_1', 0.93004)
save evaluate_step: 1
2018-11-08 13:09:33
starting shuffle train data
finish building train data
epoch=1 step: 13750 loss 0.35385704 processed: [1.1]
epoch=1 step: 15000 loss 0.3656442 processed: [1.2]
epoch=1 step: 16250 loss 0.38546053 processed: [1.3]
epoch=1 step: 17500 loss 0.23952365 processed: [1.4]
epoch=1 step: 18750 loss 0.3729167 processed: [1.5]
epoch=1 step: 20000 loss 0.2674682 processed: [1.6]
epoch=1 step: 21250 loss 0.24037017 processed: [1.7]
epoch=1 step: 22500 loss 0.2990902 processed: [1.8]
epoch=1 step: 23750 loss 0.2585538 processed: [1.9]
epoch=1 step: 25000 loss 0.27999818 processed: [2.0]
('R10_1', 0.74844)
('R2_1', 0.93134)
save evaluate_step: 2
2018-11-08 16:48:09
0.27999818
epoch=1 save model
2018-11-08 16:48:21
starting shuffle train data
finish building train data
epoch=2 step: 26250 loss 0.35569516 processed: [2.1]
epoch=2 step: 27500 loss 0.30793697 processed: [2.2]
epoch=2 step: 28750 loss 0.3315224 processed: [2.3]
epoch=2 step: 30000 loss 0.33547133 processed: [2.4]
epoch=2 step: 31250 loss 0.29268742 processed: [2.5]
epoch=2 step: 32500 loss 0.23429169 processed: [2.6]
epoch=2 step: 33750 loss 0.26348406 processed: [2.7]
epoch=2 step: 35000 loss 0.29881963 processed: [2.8]
epoch=2 step: 36250 loss 0.3020438 processed: [2.9]
epoch=2 step: 37500 loss 0.36415404 processed: [3.0]
('R10_1', 0.74194)
('R2_1', 0.92824)
save evaluate_step: 3
2018-11-08 20:25:35
0.36415404
epoch=2 save model
2018-11-08 20:25:37
starting shuffle train data
finish building train data
epoch=3 step: 38750 loss 0.35174024 processed: [3.1]
epoch=3 step: 40000 loss 0.2699746 processed: [3.2]
epoch=3 step: 41250 loss 0.40759468 processed: [3.3]
epoch=3 step: 42500 loss 0.37060475 processed: [3.4]
epoch=3 step: 43750 loss 0.21322379 processed: [3.5]
epoch=3 step: 45000 loss 0.3010804 processed: [3.6]
epoch=3 step: 46250 loss 0.29013175 processed: [3.7]
epoch=3 step: 47500 loss 0.31370252 processed: [3.8]
epoch=3 step: 48750 loss 0.32126775 processed: [3.9]
epoch=3 step: 50000 loss 0.29867923 processed: [4.0]
('R10_1', 0.7436)
('R2_1', 0.92826)
save evaluate_step: 4
2018-11-09 00:02:25
0.29867926
epoch=3 save model
2018-11-09 00:02:27
starting shuffle train data
finish building train data
epoch=4 step: 51250 loss 0.22220054 processed: [4.1]
epoch=4 step: 52500 loss 0.3729023 processed: [4.2]
epoch=4 step: 53750 loss 0.385149 processed: [4.3]
epoch=4 step: 55000 loss 0.29878917 processed: [4.4]
epoch=4 step: 56250 loss 0.3196586 processed: [4.5]
epoch=4 step: 57500 loss 0.24002375 processed: [4.6]
epoch=4 step: 58750 loss 0.22136612 processed: [4.7]
epoch=4 step: 60000 loss 0.22785321 processed: [4.8]
epoch=4 step: 61250 loss 0.30577335 processed: [4.9]
epoch=4 step: 62500 loss 0.42045164 processed: [5.0]
('R10_1', 0.74142)
('R2_1', 0.92846)
save evaluate_step: 5
2018-11-09 03:38:48
0.42045164
epoch=4 save model
2018-11-09 03:38:49
train time:18.3521 h
